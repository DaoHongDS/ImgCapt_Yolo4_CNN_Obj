{"cells":[{"cell_type":"markdown","metadata":{"id":"1offXqrEdW-C"},"source":["sửa bleu thành corpus_bleu"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24188,"status":"ok","timestamp":1718439073773,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"wlK2GHmq9gvX","outputId":"21590ba7-9f95-40c8-ee7c-dfc926ae24fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":521,"status":"ok","timestamp":1700972101067,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"Bw80GFd79yyi","outputId":"6245a824-5819-4845-9164-a26c70092e66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sun Nov 26 04:14:14 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   51C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7956,"status":"ok","timestamp":1700972111778,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"fPiGhF-y9zLZ","outputId":"423c10b6-04da-4fb5-f01a-040af57dfb85"},"outputs":[{"name":"stdout","output_type":"stream","text":["'/content/drive/MyDrive/coco/pycocotools' -> '/content/pycocotools'\n","'/content/drive/MyDrive/coco/pycocotools/_mask.pyx' -> '/content/pycocotools/_mask.pyx'\n","'/content/drive/MyDrive/coco/pycocotools/__init__.py' -> '/content/pycocotools/__init__.py'\n","'/content/drive/MyDrive/coco/pycocotools/_mask.c' -> '/content/pycocotools/_mask.c'\n","'/content/drive/MyDrive/coco/pycocotools/_mask.cp39-win_amd64.pyd' -> '/content/pycocotools/_mask.cp39-win_amd64.pyd'\n","'/content/drive/MyDrive/coco/pycocotools/mask.py' -> '/content/pycocotools/mask.py'\n","'/content/drive/MyDrive/coco/pycocotools/coco.py' -> '/content/pycocotools/coco.py'\n","'/content/drive/MyDrive/coco/pycocotools/__pycache__' -> '/content/pycocotools/__pycache__'\n","'/content/drive/MyDrive/coco/pycocotools/__pycache__/coco.cpython-39.pyc' -> '/content/pycocotools/__pycache__/coco.cpython-39.pyc'\n","'/content/drive/MyDrive/coco/pycocotools/__pycache__/__init__.cpython-39.pyc' -> '/content/pycocotools/__pycache__/__init__.cpython-39.pyc'\n","'/content/drive/MyDrive/coco/pycocotools/__pycache__/mask.cpython-39.pyc' -> '/content/pycocotools/__pycache__/mask.cpython-39.pyc'\n","'/content/drive/MyDrive/coco/pycocotools/cocoeval.py' -> '/content/pycocotools/cocoeval.py'\n"]}],"source":["!cp -av '/content/drive/MyDrive/coco/pycocotools' '/content/'\n","\n","# change the current working directory to the parent directory of this code file # hadi\n","import os\n","os.chdir('/content/drive/MyDrive/YOLOv4_CNN_RNN')  # hadie"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yVZ7cQIr-C6Z"},"outputs":[],"source":["import tensorflow as tf\n","tf.get_logger().setLevel('ERROR')  # hadie\n","# You'll generate plots of attention in order to see which parts of an image\n","# our model focuses on  during captioning\n","\n","import matplotlib.pyplot as plt\n","from pycocotools.coco import COCO\n","\n","# Scikit-learn includes many helpful utilities\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","import re\n","import numpy as np\n","import os\n","import time\n","import json\n","from glob import glob\n","from PIL import Image\n","import pickle\n","\n","from tqdm import tqdm  # hadie\n","from termcolor import colored  # hadie\n","from builtins import len  # hadie\n","import datetime  # hadie\n","\n","from model import DATASET, EXAMPLE_NUMBER, LIMIT_SIZE , WORD_DICT_SIZE, TEST_SET_PROPORTION, MY_EMBEDDING_DIM, UNIT_COUNT, CNN_Encoder, RNN_Decoder, MY_OPTIMIZER, MY_LOSS_OBJECT, REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN, EPOCH_COUNT, feature_extraction_model, split  # hadie\n","from timeit import default_timer as timer  # hadie\n","import threading  # hadie\n","\n","# from yolo import image_path_to_yolo_bounding_boxes  # hadie\n","import cv2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4150,"status":"ok","timestamp":1700972140643,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"C0EZhqgFKKtt","outputId":"ddf13a98-45b3-48ec-f7eb-8c982d95d2e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading annotations into memory...\n","Done (t=2.92s)\n","creating index...\n","index created!\n","['/content/drive/MyDrive/coco/train2014/COCO_train2014_000000197705.jpg']\n","[197705]\n"]}],"source":["# Hong\n","# 1. Load COCO training data\n","\n","PATH ='/content/drive/MyDrive/coco/train2014/'\n","annotation_file = \"/content/drive/MyDrive/coco/annotations/captions_train2014.json\"\n","\n","coco = COCO(annotation_file)\n","all_Ids = coco.getImgIds()\n","input_ids = shuffle(all_Ids, random_state=0)\n","\n","num_examples = 20000\n","input_ids = input_ids[:num_examples]\n","\n","input_img_name = []\n","for i in range(len(input_ids)):\n","    input_img_name.append(PATH+'COCO_train2014_' + '%012d' % (input_ids[i]) + '.jpg')\n","\n","### input_data = {img_name_train[i]: [cap_1, cap_2, ... cap_5]}\n","input_data = {}\n","for i in range(0,len(input_img_name)):\n","    if input_img_name[i] not in input_data:\n","        annIds = coco.getAnnIds(imgIds = input_ids[i])\n","        anns = coco.loadAnns(annIds)\n","        captions = []\n","        for j in range(0,len(anns)):\n","            a = '<start> '+ anns[j]['caption'] + ' <end>'\n","            captions.append(a.lower())\n","        input_data[input_img_name[i]] = captions\n","\n","print(input_img_name[:1])\n","print(input_ids[:1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8500,"status":"ok","timestamp":1700972154143,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"tzTlHGylDe-R","outputId":"78ade2df-02de-4637-cde6-0f40ce2cdc9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n","83683744/83683744 [==============================] - 5s 0us/step\n"]}],"source":["\"\"\"## Preprocess the images\n","\"\"\"\n","\n","# the load_image function has been moved to a separate file by hadie\n","\n","import importlib  # hadie\n","mod = importlib.import_module(\"feature_extraction_model_\" + feature_extraction_model)  # hadie\n","image_model = mod.image_model  # hadie\n","load_image = mod.load_image  # hadie\n","attention_features_shape = mod.attention_features_shape + 1  # hadie"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2NLeoLh2GVmu"},"outputs":[],"source":["\"\"\"\n","\"\"\"\n","\n","# the image_model variable has been moved to a separate file by hadie\n","\n","new_input = image_model.input # (none, none, none, 3)\n","hidden_layer = image_model.layers[-1].output # (None, None, None, 2048)\n","\n","image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":291,"status":"ok","timestamp":1699806265007,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"25HNzAANFml_","outputId":"aafda448-76d6-40e1-edff-d126f2021870"},"outputs":[{"name":"stdout","output_type":"stream","text":["Params for Xception  20861480\n"]}],"source":["print('Params for Xception ',image_features_extract_model.count_params())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10249,"status":"ok","timestamp":1700972172879,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"vZlPUliZuMQr","outputId":"899d7fec-7556-4610-9e07-756c3a8fe261"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-8-130f2b41b8a3>:1: DeprecationWarning: Please use `argsort` from the `scipy.linalg` namespace, the `scipy.linalg.decomp` namespace is deprecated.\n","  from scipy.linalg.decomp import argsort\n"]}],"source":["from scipy.linalg.decomp import argsort\n","from IPython.utils.text import indent\n","\n","# Yolo model\n","net = cv2.dnn.readNet('yolov4.weights', 'yolov4.cfg')\n","scale = 0.00392 #=1/255\n","layer_names = ['yolo_139','yolo_150','yolo_161'] # 3 head layers\n","\n","def extract_yolo_features(filename):\n","\n","    # net = cv2.dnn.readNet('yolov4.weights', 'yolov4.cfg')\n","    # scale = 0.00392 #=1/255\n","\n","    image = cv2.imread(filename)\n","\n","    blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n","    net.setInput(blob)\n","\n","    # layer_names = ['yolo_139','yolo_150','yolo_161'] # 3 head layers\n","    layers = net.forward(layer_names)\n","\n","    return layers\n","\n","\n","def extract_combined_feature(filename):\n","\n","    # 1. Xception feature\n","    img, _ = load_image(filename)\n","    img = np.expand_dims(img, axis=0)\n","    # print('img.shape',img.shape) # (1, 299, 299, 3)\n","    batch_features = image_features_extract_model(img)\n","    # print('batch_features 1', batch_features.shape) # (1, 10, 10, 2048)\n","    batch_features = tf.reshape(batch_features,\n","                                (-1, batch_features.shape[3]))\n","    # print('batch_features 2', batch_features.shape) # (100, 2048)\n","\n","    # 2. Yolo objects\n","    extracted_layers = extract_yolo_features(filename)\n","\n","    # concat 3 output layers\n","    out = np.concatenate((extracted_layers[0], extracted_layers[1], extracted_layers[2]), axis = 0)\n","\n","    ### NMS to get bounding boxes\n","\n","    image = cv2.imread(filename)\n","    # Width = image.shape[1]\n","    # Height = image.shape[0]\n","\n","    class_ids=[]\n","    confidences=[]\n","    p = []\n","    boxes=[]\n","    conf_threshold, nms_threshold = 0.5, 0.4\n","\n","    for detection in out:\n","        scores = detection[5:]\n","        class_id = np.argmax(scores)\n","        confidence = scores[class_id] # prop of class\n","        if confidence > conf_threshold:\n","            # center_x = int(detection[0] * Width)\n","            # center_y = int(detection[1] * Height)\n","            # w = int(detection[2] * Width)\n","            # h = int(detection[3] * Height)\n","            # x = center_x - w / 2\n","            # y = center_y - h / 2\n","\n","            p.append(detection[4])\n","\n","            class_ids.append(class_id)\n","            confidences.append(float(confidence))\n","            boxes.append([detection[0], detection[1], detection[2], detection[3]])\n","\n","    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n","    # print('indices: ', indices)\n","\n","    bounding_boxes=[]\n","    important=[]\n","    for i in range(len(list(indices))):\n","        # boxes = [x, y, w, h, p, class_ids, important_factor]\n","\n","        ########## can takeover this 4 position values to save more [class_id, important_factor]\n","        bbx = boxes[indices[i]].copy()\n","\n","        bbx.append(p[indices[i]])\n","\n","        important_factor = boxes[indices[i]][2] * boxes[indices[i]][3] * confidences[indices[i]]\n","        important.append(important_factor)\n","        bbx.append(class_ids[indices[i]])\n","        bbx.append(important_factor)\n","\n","        bounding_boxes.append(bbx)\n","\n","    ### sort bounding box to get the most important bounding box first\n","\n","    ranked = np.argsort(np.array(important))\n","    largest_indices = ranked[::-1]\n","\n","    sorted_bbx=[]\n","    for i in largest_indices:\n","        b = bounding_boxes[i].copy()\n","        sorted_bbx.append(b)\n","    # print('sorted_bbx',sorted_bbx)\n","\n","    # 3. Combine feature and objects\n","\n","    # flattening and padding to fit size of feature vector\n","\n","    features_shape = batch_features.shape[1] # 2048\n","    # print('features_shape',features_shape) # 2048\n","\n","    yolo_features = np.array(sorted_bbx).flatten()\n","    # print('shape of yolo_features', yolo_features.shape)\n","\n","    # get the first num_of_box to fit the size of feature_shape\n","    num_of_box = features_shape//7          #\n","\n","    yolo_features = yolo_features[:num_of_box*7]\n","    # print('yolo_features.shape ', yolo_features.shape)\n","    yolo_features = np.pad(yolo_features, (0, features_shape - yolo_features.shape[0]), 'constant', constant_values=(0, 0)).astype(np.float32)\n","    yolo_features = yolo_features.reshape(-1, yolo_features.shape[0])\n","    # print('yolo_features.shape',yolo_features.shape)\n","\n","    feature = np.concatenate((batch_features, yolo_features), axis=0) # 101x2048\n","    # print('combined feature shape', feature.shape)\n","\n","    return feature"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2368,"status":"ok","timestamp":1700973413421,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"fiikuHDJO4ba","outputId":"437eb022-be9a-4b6a-9123-6ef375e072d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------------------------START OF EXECUTION-----------------------------\n","extracting features (0) file(s)\n","len of extracting 0\n","\n","finished extracting features\n"]}],"source":["# extract combined feature for each image\n","\n","print(\"-----------------------------START OF EXECUTION-----------------------------\")  # hadie\n","\n","feature_dir = './3_Yolo4_Xception_RNN/xception_npy'\n","\n","encode_train = [x for x in input_img_name if not os.path.exists(feature_dir + \"/\" + x[-31:-4] + \".npy\")]\n","# features_shape = 2048\n","print(\"extracting features (\" + str(len(encode_train)) + \") file(s)\")\n","print('len of extracting',len (encode_train))\n","\n","if len(encode_train) > 0:\n","    image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n","    image_dataset = image_dataset.batch(16)\n","\n","    for paths in tqdm(image_dataset):\n","        for path in paths:\n","            # print('path', path)\n","            fn = path.numpy().decode(\"utf-8\")\n","            combined_feature = extract_combined_feature(fn)\n","            np.save(feature_dir + \"/\" + fn[-31:-4], combined_feature)\n","\n","print(\"\\nfinished extracting features\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1948,"status":"ok","timestamp":1700973419157,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"IUrIMN122Q3h","outputId":"dc41d561-7dc9-49d5-eb6b-cbf910ac8195"},"outputs":[{"name":"stdout","output_type":"stream","text":["using the cashed tokenizer\n"]}],"source":["\"\"\"## Preprocess and tokenize the captions\n","\n","* First, you'll tokenize the captions (for example, by splitting on spaces).\n","This gives us a  vocabulary of all of the unique words in the data (for example, \"surfing\", \"football\", and so on).\n","* Next, you'll limit the vocabulary size to the top 5,000 words (to save memory).\n","You'll replace all other words with the token \"UNK\" (unknown).\n","* You then create word-to-index and index-to-word mappings.\n","* Finally, you pad all sequences to be the same length as the longest one.\n","\"\"\"\n","# Hong\n","def dict_to_list(train_descriptions):\n","    all_desc = []\n","    for key in train_descriptions.keys():\n","        [all_desc.append(d) for d in train_descriptions[key]]\n","    return all_desc\n","\n","# Find the maximum length of any caption in our dataset\n","def calc_max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","\n","# Choose the top 5000 words from the vocabulary\n","top_k = WORD_DICT_SIZE  # hadie 15000\n","\n","\n","\n","##################################################\n","REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN = False\n","\n","input_captions = dict_to_list(input_data) # Hong\n","\n","if not REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:  # hadie\n","    print(\"using the cashed tokenizer\")  # hadie\n","    # loading the tokenizer # hadie\n","    with open(\"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/tokenizer.pickle\", 'rb') as handle:  # hadie\n","        tokenizer = pickle.load(handle)  # hadie\n","else:  # hadie\n","    print(\"tokenizing and padding captions\")  # hadie\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n","                                                      oov_token=\"<unk>\",\n","                                                      filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n","    tokenizer.fit_on_texts(input_captions)\n","    # train_seqs = tokenizer.texts_to_sequences(train_captions)  # 777 maybe this line needs removal\n","    tokenizer.word_index['<pad>'] = 0\n","    tokenizer.index_word[0] = '<pad>'\n","    # saving the tokenizer to disk # hadie\n","    with open(\"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/tokenizer.pickle\", 'wb') as handle:  # hadie\n","        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)  # hadie\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5eKvn0PK2mV"},"outputs":[],"source":["# Create training and validation sets using an 80-20 split\n","PROP = 0.1 # hong\n","\n","if PROP == 0:\n","    image_id_train = input_ids\n","    image_id_val = []\n","    img_name_train = input_img_name\n","    img_name_val = []\n","    # cap_train = cap_vector\n","    # cap_val = []\n","elif PROP == 1:\n","    image_id_train = []\n","    image_id_val = input_ids\n","    img_name_train = []\n","    img_name_val = input_img_name\n","    # cap_train = []\n","    # cap_val = cap_vector\n","else:\n","    image_id_train, image_id_val, img_name_train_2, img_name_val_2 = train_test_split(\n","                                                                    input_ids,  # hadie\n","                                                                    input_img_name,\n","                                                                    test_size=PROP,  # hadie\n","                                                                    random_state=0)\n","\n","cap_train_list = []\n","cap_val_list = []\n","for i in range(len(img_name_train_2)):\n","    for j in range(len(input_data[img_name_train_2[i]])):\n","        cap_train_list.append(input_data[img_name_train_2[i]][j])\n","for i in range(len(img_name_val_2)):\n","    for j in range(len(input_data[img_name_val_2[i]])):\n","        cap_val_list.append(input_data[img_name_val_2[i]][j])\n","\n","train_seqs = tokenizer.texts_to_sequences(cap_train_list)\n","val_seqs = tokenizer.texts_to_sequences(cap_val_list)\n","cap_train = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n","cap_val = tf.keras.preprocessing.sequence.pad_sequences(val_seqs, padding='post')\n","\n","# Hong\n","def dict_to_list_2(dict_data, list_key):\n","    all_img_name = []\n","    for i in range(len(list_key)):\n","        if list_key[i] in dict_data:\n","            for j in range(len(dict_data[list_key[i]])):\n","                all_img_name.append(list_key[i])\n","    return all_img_name\n","\n","img_name_train = dict_to_list_2(input_data, img_name_train_2)\n","img_name_val = dict_to_list_2(input_data, img_name_val_2)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1012,"status":"ok","timestamp":1700973429684,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"CvHnf-zaK8vM","outputId":"3bc70e7e-cc4b-4253-a200-ffcd5ee3b710"},"outputs":[{"name":"stdout","output_type":"stream","text":["finished tokenizing and padding captions\n"]}],"source":["if not REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:  # hadie\n","    file = \"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/max_length.txt\"  # hadie\n","    with open(file, 'r') as filetoread:  # hadie\n","        max_length = int(filetoread.readline())  # hadie\n","else:  # hadie\n","    # Calculates the max_length, which is used to store the attention weights\n","    max_length = calc_max_length(train_seqs)\n","\n","    file = \"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/max_length.txt\"  # hadie\n","    with open(file, 'w') as filetowrite:  # hadie\n","        filetowrite.write(str(max_length))  # write the maximum length to disk # hadie\n","\n","print(\"finished tokenizing and padding captions\")  # hadie"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNf3NlRT-BSF"},"outputs":[],"source":["\"\"\"## Create a tf.data dataset for training\n","\"\"\"\n","\n","# Feel free to change these parameters according to your system's configuration\n","\n","BATCH_SIZE = 8*5  # 64\n","# BUFFER_SIZE = 10000\n","embedding_dim = MY_EMBEDDING_DIM  # hadie 256\n","units = UNIT_COUNT  # hadie 512\n","vocab_size = top_k + 1 # 15001\n","num_steps = len(img_name_train) // BATCH_SIZE\n","# Shape of the vector extracted from InceptionV3 is (64, 2048)\n","# These two variables represent that vector shape\n","# the definition of features_shape = 2048 was moved up\n","# the attention_features variable has been moved to a separate file by hadie\n","\n","\n","# Load the numpy files\n","def map_func(img_name, cap):\n","  img_tensor = np.load(feature_dir + '/' + img_name.decode('utf-8')[-31:-4] + '.npy')\n","  return img_tensor, cap\n","\n","\n","dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n","dataset = dataset.shuffle(BATCH_SIZE * 8)\n","\n","dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),\n","          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","dataset = dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hb1rumUy-WET"},"outputs":[],"source":["\"\"\"## Model\n","\n","\"\"\"\n","\n","# the model has been moved to model.py by hadie\n","\n","encoder = CNN_Encoder(embedding_dim)\n","decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n","\n","optimizer = MY_OPTIMIZER  # hadie\n","loss_object = MY_LOSS_OBJECT  # hadie\n","\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)\n","\n","\n","\"\"\"## Checkpoint\"\"\"\n","\n","checkpoint_path = \"./3_Yolo4_Xception_RNN/checkpoint\"\n","\n","if REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:\n","    try:\n","        for filename in os.listdir(checkpoint_path):\n","            print(\"deleting \" + checkpoint_path + \"/\" + filename)\n","            os.unlink(checkpoint_path + \"/\" + filename)\n","    except Exception as e:\n","        print('Failed to delete %s. Reason: %s' % (checkpoint_path + \"/\" + filename, e))\n","    # remove the saved model too\n","    if os.path.exists(\"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/my_model.index\"):\n","        print(\"deleting trained_model_\" + feature_extraction_model + \"/my_model.index\")\n","        os.unlink(\"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/my_model.index\")\n","    if os.path.exists(\"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/checkpoint\"):\n","        print(\"deleting /trained_model_\" + feature_extraction_model + \"/checkpoint\")\n","        os.unlink(\"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/checkpoint\")\n","    if os.path.exists(\"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/my_model.data-00000-of-00001\"):\n","        print(\"deleting trained_model_\" + feature_extraction_model + \"/my_model.data-00000-of-00001\")\n","        os.unlink(\"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/my_model.data-00000-of-00001\")\n","    if os.path.exists(\"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/learning_curve.png\"):\n","        print(\"deleting trained_model_\" + feature_extraction_model + \"/learning_curve.png\")\n","        os.unlink(\"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/learning_curve.png\")\n","\n","ckpt = tf.train.Checkpoint(encoder=encoder,\n","                           decoder=decoder,\n","                           optimizer=optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","start_epoch = 0\n","if ckpt_manager.latest_checkpoint:\n","    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5278349,"status":"ok","timestamp":1700978721776,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"vh8hcq8s_Gr9","outputId":"84db2e85-47e4-43e5-bf73-486036a63c11"},"outputs":[{"name":"stdout","output_type":"stream","text":["training..\n","Epoch 3 Batch 0 Loss 0.6997\n","Epoch 3 Batch 100 Loss 0.6033\n","Epoch 3 Batch 200 Loss 0.6791\n","Epoch 3 Batch 300 Loss 0.6650\n","Epoch 3 Batch 400 Loss 0.7420\n","Epoch 3 Batch 500 Loss 0.6520\n","Epoch 3 Batch 600 Loss 0.6604\n","Epoch 3 Batch 700 Loss 0.5789\n","Epoch 3 Batch 800 Loss 0.5833\n","Epoch 3 Batch 900 Loss 0.5264\n","Epoch 3 Batch 1000 Loss 0.5876\n","Epoch 3 Batch 1100 Loss 0.5303\n","Epoch 3 Batch 1200 Loss 0.5545\n","Epoch 3 Batch 1300 Loss 0.5847\n","Epoch 3 Batch 1400 Loss 0.5767\n","Epoch 3 Batch 1500 Loss 0.5941\n","Epoch 3 Batch 1600 Loss 0.6301\n","Epoch 3 Batch 1700 Loss 0.5621\n","Epoch 3 Batch 1800 Loss 0.6025\n","Epoch 3 Batch 1900 Loss 0.6151\n","Epoch 3 Batch 2000 Loss 0.5652\n","Epoch 3 Batch 2100 Loss 0.6030\n","Epoch 3 Batch 2200 Loss 0.5574\n","Epoch 3 Loss 0.611892\n","Time taken for 1 epoch 2246.9666912555695 sec\n","\n","Epoch 4 Batch 0 Loss 0.6412\n","Epoch 4 Batch 100 Loss 0.6377\n","Epoch 4 Batch 200 Loss 0.5568\n","Epoch 4 Batch 300 Loss 0.6252\n","Epoch 4 Batch 400 Loss 0.6006\n","Epoch 4 Batch 500 Loss 0.6424\n","Epoch 4 Batch 600 Loss 0.5225\n","Epoch 4 Batch 700 Loss 0.5941\n","Epoch 4 Batch 800 Loss 0.5957\n","Epoch 4 Batch 900 Loss 0.5122\n","Epoch 4 Batch 1000 Loss 0.5837\n","Epoch 4 Batch 1100 Loss 0.5770\n","Epoch 4 Batch 1200 Loss 0.5716\n","Epoch 4 Batch 1300 Loss 0.6215\n","Epoch 4 Batch 1400 Loss 0.5349\n","Epoch 4 Batch 1500 Loss 0.5482\n","Epoch 4 Batch 1600 Loss 0.5012\n","Epoch 4 Batch 1700 Loss 0.5537\n","Epoch 4 Batch 1800 Loss 0.5647\n","Epoch 4 Batch 1900 Loss 0.5256\n","Epoch 4 Batch 2000 Loss 0.5997\n","Epoch 4 Batch 2100 Loss 0.6976\n","Epoch 4 Batch 2200 Loss 0.5768\n","Epoch 4 Loss 0.571402\n","Time taken for 1 epoch 433.6068334579468 sec\n","\n","Epoch 5 Batch 0 Loss 0.5380\n","Epoch 5 Batch 100 Loss 0.6092\n","Epoch 5 Batch 200 Loss 0.5186\n","Epoch 5 Batch 300 Loss 0.5696\n","Epoch 5 Batch 400 Loss 0.5900\n","Epoch 5 Batch 500 Loss 0.5329\n","Epoch 5 Batch 600 Loss 0.5381\n","Epoch 5 Batch 700 Loss 0.5612\n","Epoch 5 Batch 800 Loss 0.5151\n","Epoch 5 Batch 900 Loss 0.5601\n","Epoch 5 Batch 1000 Loss 0.5792\n","Epoch 5 Batch 1100 Loss 0.5718\n","Epoch 5 Batch 1200 Loss 0.5109\n","Epoch 5 Batch 1300 Loss 0.5486\n","Epoch 5 Batch 1400 Loss 0.5321\n","Epoch 5 Batch 1500 Loss 0.5501\n","Epoch 5 Batch 1600 Loss 0.5253\n","Epoch 5 Batch 1700 Loss 0.4964\n","Epoch 5 Batch 1800 Loss 0.5071\n","Epoch 5 Batch 1900 Loss 0.4407\n","Epoch 5 Batch 2000 Loss 0.5395\n","Epoch 5 Batch 2100 Loss 0.5890\n","Epoch 5 Batch 2200 Loss 0.5129\n","Epoch 5 Loss 0.537312\n","Time taken for 1 epoch 432.850704908371 sec\n","\n","Epoch 6 Batch 0 Loss 0.4943\n","Epoch 6 Batch 100 Loss 0.5494\n","Epoch 6 Batch 200 Loss 0.4809\n","Epoch 6 Batch 300 Loss 0.5794\n","Epoch 6 Batch 400 Loss 0.4965\n","Epoch 6 Batch 500 Loss 0.5289\n","Epoch 6 Batch 600 Loss 0.4590\n","Epoch 6 Batch 700 Loss 0.4727\n","Epoch 6 Batch 800 Loss 0.4978\n","Epoch 6 Batch 900 Loss 0.4626\n","Epoch 6 Batch 1000 Loss 0.4609\n","Epoch 6 Batch 1100 Loss 0.5672\n","Epoch 6 Batch 1200 Loss 0.5147\n","Epoch 6 Batch 1300 Loss 0.5011\n","Epoch 6 Batch 1400 Loss 0.5334\n","Epoch 6 Batch 1500 Loss 0.5321\n","Epoch 6 Batch 1600 Loss 0.4927\n","Epoch 6 Batch 1700 Loss 0.5186\n","Epoch 6 Batch 1800 Loss 0.4506\n","Epoch 6 Batch 1900 Loss 0.4418\n","Epoch 6 Batch 2000 Loss 0.5438\n","Epoch 6 Batch 2100 Loss 0.4944\n","Epoch 6 Batch 2200 Loss 0.4682\n","Epoch 6 Loss 0.507871\n","Time taken for 1 epoch 432.8698127269745 sec\n","\n","Epoch 7 Batch 0 Loss 0.5075\n","Epoch 7 Batch 100 Loss 0.4862\n","Epoch 7 Batch 200 Loss 0.4801\n","Epoch 7 Batch 300 Loss 0.5133\n","Epoch 7 Batch 400 Loss 0.5280\n","Epoch 7 Batch 500 Loss 0.4537\n","Epoch 7 Batch 600 Loss 0.4880\n","Epoch 7 Batch 700 Loss 0.5129\n","Epoch 7 Batch 800 Loss 0.4926\n","Epoch 7 Batch 900 Loss 0.5030\n","Epoch 7 Batch 1000 Loss 0.4500\n","Epoch 7 Batch 1100 Loss 0.4367\n","Epoch 7 Batch 1200 Loss 0.4403\n","Epoch 7 Batch 1300 Loss 0.4860\n","Epoch 7 Batch 1400 Loss 0.4975\n","Epoch 7 Batch 1500 Loss 0.5305\n","Epoch 7 Batch 1600 Loss 0.4796\n","Epoch 7 Batch 1700 Loss 0.4385\n","Epoch 7 Batch 1800 Loss 0.4569\n","Epoch 7 Batch 1900 Loss 0.4506\n","Epoch 7 Batch 2000 Loss 0.6085\n","Epoch 7 Batch 2100 Loss 0.5513\n","Epoch 7 Batch 2200 Loss 0.4134\n","Epoch 7 Loss 0.481686\n","Time taken for 1 epoch 432.937194108963 sec\n","\n","Epoch 8 Batch 0 Loss 0.4895\n","Epoch 8 Batch 100 Loss 0.5286\n","Epoch 8 Batch 200 Loss 0.4621\n","Epoch 8 Batch 300 Loss 0.5172\n","Epoch 8 Batch 400 Loss 0.4440\n","Epoch 8 Batch 500 Loss 0.4975\n","Epoch 8 Batch 600 Loss 0.4391\n","Epoch 8 Batch 700 Loss 0.4440\n","Epoch 8 Batch 800 Loss 0.4887\n","Epoch 8 Batch 900 Loss 0.4382\n","Epoch 8 Batch 1000 Loss 0.4122\n","Epoch 8 Batch 1100 Loss 0.4308\n","Epoch 8 Batch 1200 Loss 0.4017\n","Epoch 8 Batch 1300 Loss 0.4905\n","Epoch 8 Batch 1400 Loss 0.5026\n","Epoch 8 Batch 1500 Loss 0.4249\n","Epoch 8 Batch 1600 Loss 0.4122\n","Epoch 8 Batch 1700 Loss 0.4547\n","Epoch 8 Batch 1800 Loss 0.5484\n","Epoch 8 Batch 1900 Loss 0.4353\n","Epoch 8 Batch 2000 Loss 0.4443\n","Epoch 8 Batch 2100 Loss 0.5104\n","Epoch 8 Batch 2200 Loss 0.4220\n","Epoch 8 Loss 0.458353\n","Time taken for 1 epoch 432.7106330394745 sec\n","\n","Epoch 9 Batch 0 Loss 0.4702\n","Epoch 9 Batch 100 Loss 0.4531\n","Epoch 9 Batch 200 Loss 0.4237\n","Epoch 9 Batch 300 Loss 0.4623\n","Epoch 9 Batch 400 Loss 0.4824\n","Epoch 9 Batch 500 Loss 0.4475\n","Epoch 9 Batch 600 Loss 0.3954\n","Epoch 9 Batch 700 Loss 0.4232\n","Epoch 9 Batch 800 Loss 0.4513\n","Epoch 9 Batch 900 Loss 0.4424\n","Epoch 9 Batch 1000 Loss 0.4600\n","Epoch 9 Batch 1100 Loss 0.4367\n","Epoch 9 Batch 1200 Loss 0.4541\n","Epoch 9 Batch 1300 Loss 0.4208\n","Epoch 9 Batch 1400 Loss 0.4180\n","Epoch 9 Batch 1500 Loss 0.4366\n","Epoch 9 Batch 1600 Loss 0.3947\n","Epoch 9 Batch 1700 Loss 0.4563\n","Epoch 9 Batch 1800 Loss 0.4520\n","Epoch 9 Batch 1900 Loss 0.3536\n","Epoch 9 Batch 2000 Loss 0.4612\n","Epoch 9 Batch 2100 Loss 0.4271\n","Epoch 9 Batch 2200 Loss 0.4373\n","Epoch 9 Loss 0.437508\n","Time taken for 1 epoch 432.91487288475037 sec\n","\n","Epoch 10 Batch 0 Loss 0.4444\n","Epoch 10 Batch 100 Loss 0.3727\n","Epoch 10 Batch 200 Loss 0.4328\n","Epoch 10 Batch 300 Loss 0.5117\n","Epoch 10 Batch 400 Loss 0.4454\n","Epoch 10 Batch 500 Loss 0.4159\n","Epoch 10 Batch 600 Loss 0.4325\n","Epoch 10 Batch 700 Loss 0.4524\n","Epoch 10 Batch 800 Loss 0.4177\n","Epoch 10 Batch 900 Loss 0.4067\n","Epoch 10 Batch 1000 Loss 0.4038\n","Epoch 10 Batch 1100 Loss 0.3660\n","Epoch 10 Batch 1200 Loss 0.4491\n","Epoch 10 Batch 1300 Loss 0.4135\n","Epoch 10 Batch 1400 Loss 0.4185\n","Epoch 10 Batch 1500 Loss 0.3757\n","Epoch 10 Batch 1600 Loss 0.4466\n","Epoch 10 Batch 1700 Loss 0.3808\n","Epoch 10 Batch 1800 Loss 0.4258\n","Epoch 10 Batch 1900 Loss 0.3856\n","Epoch 10 Batch 2000 Loss 0.3947\n","Epoch 10 Batch 2100 Loss 0.4210\n","Epoch 10 Batch 2200 Loss 0.4055\n","Epoch 10 Loss 0.418845\n","Time taken for 1 epoch 432.65644574165344 sec\n","\n","Time of training:  5277.51589846611\n"]}],"source":["\"\"\"## Training\n","\n","\"\"\"\n","\n","# adding this in a separate cell because if you run the training cell\n","# many times, the loss_plot array will be reset\n","loss_plot = []\n","\n","\n","@tf.function\n","def train_step(img_tensor, target):\n","  loss = 0\n","\n","  # initializing the hidden state for each batch\n","  # because the captions are not related from image to image\n","  hidden = decoder.reset_state(batch_size=target.shape[0])\n","\n","  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n","\n","  with tf.GradientTape() as tape:\n","      features = encoder(img_tensor)\n","    #   print('Count params of encoder: ',encoder.count_params())\n","\n","      for i in range(1, target.shape[1]):\n","          # passing the features through the decoder\n","          predictions, hidden, _ = decoder(dec_input, features, hidden)\n","        #   print('Count params of decoder', decoder.count_params())\n","\n","          loss += loss_function(target[:, i], predictions)\n","\n","          # using teacher forcing\n","          dec_input = tf.expand_dims(target[:, i], 1)\n","\n","  total_loss = (loss / int(target.shape[1]))\n","\n","  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  gradients = tape.gradient(loss, trainable_variables)\n","\n","  optimizer.apply_gradients(zip(gradients, trainable_variables))\n","\n","  return loss, total_loss\n","\n","\n","# EPOCHS = EPOCH_COUNT  # hadie\n","EPOCHS = 10\n","\n","if not os.path.exists(\"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/my_model.index\"):  # hadie\n","    print(\"training..\")  # hadie\n","\n","    t1 = time.time()\n","    for epoch in range(start_epoch, EPOCHS):\n","        start = time.time()\n","        total_loss = 0\n","\n","        for (batch, (img_tensor, target)) in enumerate(dataset):\n","            # print('img_tensor.shape', img_tensor.shape)\n","            # print('target.shape', target.shape)\n","            batch_loss, t_loss = train_step(img_tensor, target)\n","            total_loss += t_loss\n","\n","            if batch % 100 == 0:\n","                print ('Epoch {} Batch {} Loss {:.4f}'.format(\n","                  epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n","        # storing the epoch end loss value to plot later\n","        loss_plot.append(total_loss / num_steps)\n","\n","        # if epoch % 5 == 0:\n","        #   ckpt_manager.save()\n","        ckpt_manager.save()\n","\n","        print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n","                                             total_loss / num_steps))\n","        print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","\n","    t2 = time.time()\n","    # save model to disk # hadie\n","    decoder.save_weights(\"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/my_model\" , save_format=\"tf\")  # hadie\n","    print('Time of training: ', t2-t1)\n","else:  # hadie\n","    print(\"A trained model has been found. Loading it from disk..\")  # hadie\n","    # Load the previously saved weights # hadie\n","    decoder.load_weights(\"./3_Yolo4_Xception_RNN/trained_model_\" + feature_extraction_model + \"/my_model\")  # hadie\n","\n","# showing the learning curve is moved from here by hadie"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"12E_sg3j5QrAcHnTHRue7SK-JkIc3M-yv"},"executionInfo":{"elapsed":36347,"status":"ok","timestamp":1700979531833,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"8OT3tu5M_t6o","outputId":"f2bb6deb-ecb0-43ec-8380-168d3b0ac592"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["\"\"\"## Caption!\n","\n","\"\"\"\n","\n","attention_features_shape = 101\n","def evaluate(image):\n","    attention_plot = np.zeros((max_length, attention_features_shape))\n","\n","    hidden = decoder.reset_state(batch_size=1)\n","\n","    combined_features = extract_combined_feature(image)\n","    features = encoder(combined_features)  # hadie\n","\n","    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n","\n","    result = []\n","\n","    for i in range(max_length):\n","        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n","        # print('predictions', predictions)\n","\n","        attention_plot[i] = tf.reshape(attention_weights, (-1,)).numpy()\n","        # print('attention_plot[i]', attention_plot[i])\n","\n","        # predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","        predicted_id = np.argmax(predictions.numpy())\n","        # print('predicted_id', predicted_id)\n","\n","        result.append(tokenizer.index_word[predicted_id])\n","\n","        if tokenizer.index_word[predicted_id] == '<end>':\n","            return result, attention_plot\n","\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    attention_plot = attention_plot[:len(result),:]\n","    return result, attention_plot\n","\n","\n","def plot_attention(image, result, attention_plot):\n","    temp_image = np.array(Image.open(image))\n","\n","    fig = plt.figure(figsize=(10, 10))\n","\n","    len_result = len(result)\n","    for l in range(len_result):\n","        temp_att = np.resize(attention_plot[l], (8, 8))\n","        ax = fig.add_subplot(len_result // 2, len_result // 2, l + 1)\n","        ax.set_title(result[l])\n","        img = ax.imshow(temp_image)\n","        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def list_to_dict(name_list, caption_list):\n","    name_dict = {}\n","    for i in range(len(name_list)):\n","        if name_list[i] not in name_dict:\n","            name_dict[name_list[i]] = [caption_list[i]]\n","        else:\n","            name_dict.get(name_list[i]).append(caption_list[i])\n","            # print( name_dict.get(name_list[i]))\n","    return name_dict\n","\n","# captions on the validation set\n","val_dict = list_to_dict(img_name_val, cap_val)\n","\n","\n","# # for random samples\n","# for i in range(10):\n","#     # rid = np.random.randint(0, len(img_name_val))\n","#     image = img_name_val[i*5]\n","\n","#     captions = val_dict[image]\n","#     # print('captions:', captions)\n","\n","#     result, attention_plot = evaluate(image)\n","\n","#     # print ('Real Caption:', real_caption)\n","#     print('Real captions:')\n","#     for j in range(len(captions)):\n","#         real_caption = ' '.join([tokenizer.index_word[i] for i in captions[j] if i not in [0]])\n","#         print(real_caption)\n","\n","#     print ('Prediction Caption:', ' '.join(result))\n","#     # plot_attention(image, result, attention_plot)\n","\n","#     print('img path', image)\n","\n","#     import cv2\n","#     from google.colab.patches import cv2_imshow\n","#     img = cv2.imread(image)\n","#     cv2_imshow(img)\n","\n","\n","# for some first samples\n","\n","list_img = [\n","    '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000127298.jpg',\n","    '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000031135.jpg',\n","    '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000448859.jpg',\n","    '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000215718.jpg',\n","    '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000532398.jpg',\n","    '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000567901.jpg',\n","    '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000207691.jpg',\n","    '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000292135.jpg',\n","    '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000304973.jpg',\n","    '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000484734.jpg'\n","]\n","\n","for image in list_img:\n","    # rid = np.random.randint(0, len(img_name_val))\n","    # image = img_name_val[i*5]\n","\n","    captions = val_dict[image]\n","    # print('captions:', captions)\n","\n","    result, attention_plot = evaluate(image)\n","\n","    # print ('Real Caption:', real_caption)\n","    print('Real captions:')\n","    for j in range(len(captions)):\n","        real_caption = ' '.join([tokenizer.index_word[i] for i in captions[j] if i not in [0]])\n","        print(real_caption)\n","\n","    print ('Prediction Caption:', ' '.join(result))\n","    # plot_attention(image, result, attention_plot)\n","\n","    print('img path', image)\n","\n","    import cv2\n","    from google.colab.patches import cv2_imshow\n","    img = cv2.imread(image)\n","    cv2_imshow(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kv_wN51wrS1d"},"outputs":[],"source":["from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n","from nltk.translate.meteor_score import meteor_score\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","\n","def img_to_caption(img_name):\n","    result, attention_plot = evaluate(img_name)\n","    result = ' '.join(result).replace(\"<end>\", \"\").strip()\n","    return result\n","\n","actual, predicted = [], []\n","\n","for img_name in tqdm(val_dict):\n","\n","    yhat = img_to_caption(img_name)\n","    pred = yhat.split()\n","\n","    actu = []\n","    captions = val_dict[img_name]\n","    for i in range(len(captions)):\n","        real_caption = ' '.join([tokenizer.index_word[i] for i in captions[i] if i not in [0,3,4]])\n","        references = real_caption.split(' ')\n","        actu.append(references)\n","\n","    # print('references:', references)\n","    # print('predicted:', predicted)\n","\n","    actual.append(actu)\n","    predicted.append(pred)\n","\n","bl1 = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n","bl2 = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n","bl3 = corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))\n","bl4 = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n","\n","# print('actual:', actual)\n","# print('predicted: ' ,predicted)\n","\n","mt = 0\n","for i in range(len(predicted)):\n","    mt += round(meteor_score(actual[i], predicted[i]),4)\n","mt = mt/len(predicted)\n","# print('METEOR: %f' % mt)\n","\n","print(\"\\nbleu-1 score:\", bl1)\n","print(\"bleu-2 score:\", bl2)\n","print(\"bleu-3 score:\", bl3)\n","print(\"bleu-4 score:\", bl4)\n","print(\"meteor score:\", mt)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TIqs_Fn9eBI6"},"outputs":[],"source":["# speed test\n","\n","# for random samples\n","img_list = []\n","for i in range(100):\n","    rid = np.random.randint(0, len(img_name_val))\n","    img_list.append(img_name_val[rid])\n","\n","t1 = time.time()\n","for image in img_list:\n","    result, attention_plot = evaluate(image)\n","    predict = ' '.join(result)\n","\n","t2 = time.time()\n","print('time of predicting captions for 100 images', t2-t1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZxXnBU1OquO"},"outputs":[],"source":["# # speed test => moved to local test\n","\n","# file = open('./save/img_name_val_bk.txt', 'r')\n","# txt = file.read()\n","# file.close()\n","\n","# test_imgs1 = txt[1:-1].split(\", \")\n","# test_imgs = []\n","# num_test_file = 5\n","# for i in range(num_test_file):\n","#     test_imgs.append(test_imgs1[i][1:-1])\n","\n","# # ['/content/drive/MyDrive/coco/train2014/COCO_train2014_000000151458.jpg',\n","# # '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000419159.jpg',\n","# # '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000239584.jpg',\n","# # '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000521266.jpg',\n","# # '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000337415.jpg']\n","\n","# time_extract = 0\n","# time_predict = 0\n","\n","# def predict_caption(image):\n","\n","#     time_start_extract = time.time()\n","#     combined_features = extract_combined_feature(image)\n","#     t = time.time() - time_start_extract\n","\n","#     features = encoder(combined_features)\n","\n","#     hidden = decoder.reset_state(batch_size=1)\n","#     dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n","\n","#     result = []\n","#     for i in range(max_length):\n","#         predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n","\n","#         predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","\n","#         # hong\n","#         while predicted_id > len(tokenizer.index_word)-1:\n","#             predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","#         # hong end\n","\n","#         result.append(tokenizer.index_word[predicted_id])\n","\n","#         if tokenizer.index_word[predicted_id] == '<end>':\n","#             # print('early')\n","#             return result, t\n","\n","#         dec_input = tf.expand_dims([predicted_id], 0)\n","\n","#     return result, t\n","\n","\n","# # captions on the validation set\n","# n = len(test_imgs)\n","# for i in range(n):\n","#     image = test_imgs[i]\n","\n","#     time_start_predict = time.time()\n","#     result, t = predict_caption(image)\n","#     time_predict += time.time() - time_start_predict\n","#     time_extract += t\n","\n","#     print ('Prediction Caption:', ' '.join(result))\n","\n","#     # print('img path', image)\n","#     # import cv2\n","#     # from google.colab.patches import cv2_imshow\n","#     # img = cv2.imread(image)\n","#     # cv2_imshow(img)\n","\n","# time_extract /= n\n","# time_predict /= n\n","# print('average time_extract by Yolo_xception_RNN', time_extract)\n","# print('average time_predict by Yolo_xception_RNN', time_predict)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyP2VFSkMnYuo46oUN07BELS"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}