{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaoHongDS/ImgCapt_Yolo4_CNN_Obj/blob/main/Yolo4_RNN_conv116_512_169.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_SZJWXF_Uqh"
      },
      "source": [
        "Yolo conv116, shape 512x169 -> 512x170 -> 170x512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kcoqgZVXw9k",
        "outputId": "dce1ca20-0bf3-4fc2-a9de-938ce86ddedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEH9vcz_X44v",
        "outputId": "6ca10386-db2c-4aa7-d004-63a219cc0373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oresTuTGX7S3",
        "outputId": "457f34bf-3c6a-4f4a-8b88-e309a4b8ebdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/MyDrive/coco/pycocotools' -> '/content/pycocotools'\n",
            "'/content/drive/MyDrive/coco/pycocotools/_mask.pyx' -> '/content/pycocotools/_mask.pyx'\n",
            "'/content/drive/MyDrive/coco/pycocotools/__init__.py' -> '/content/pycocotools/__init__.py'\n",
            "'/content/drive/MyDrive/coco/pycocotools/_mask.c' -> '/content/pycocotools/_mask.c'\n",
            "'/content/drive/MyDrive/coco/pycocotools/_mask.cp39-win_amd64.pyd' -> '/content/pycocotools/_mask.cp39-win_amd64.pyd'\n",
            "'/content/drive/MyDrive/coco/pycocotools/mask.py' -> '/content/pycocotools/mask.py'\n",
            "'/content/drive/MyDrive/coco/pycocotools/coco.py' -> '/content/pycocotools/coco.py'\n",
            "'/content/drive/MyDrive/coco/pycocotools/__pycache__' -> '/content/pycocotools/__pycache__'\n",
            "'/content/drive/MyDrive/coco/pycocotools/__pycache__/coco.cpython-39.pyc' -> '/content/pycocotools/__pycache__/coco.cpython-39.pyc'\n",
            "'/content/drive/MyDrive/coco/pycocotools/__pycache__/__init__.cpython-39.pyc' -> '/content/pycocotools/__pycache__/__init__.cpython-39.pyc'\n",
            "'/content/drive/MyDrive/coco/pycocotools/__pycache__/mask.cpython-39.pyc' -> '/content/pycocotools/__pycache__/mask.cpython-39.pyc'\n",
            "'/content/drive/MyDrive/coco/pycocotools/cocoeval.py' -> '/content/pycocotools/cocoeval.py'\n"
          ]
        }
      ],
      "source": [
        "!cp -av '/content/drive/MyDrive/coco/pycocotools' '/content/'\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/YOLOv4_CNN_RNN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCOYixMmY-CG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')  # hadie\n",
        "# You'll generate plots of attention in order to see which parts of an image\n",
        "# our model focuses on  during captioning\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "# Scikit-learn includes many helpful utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle\n",
        "\n",
        "from tqdm import tqdm  # hadie\n",
        "from termcolor import colored  # hadie\n",
        "from builtins import len  # hadie\n",
        "import datetime  # hadie\n",
        "\n",
        "from model import DATASET, EXAMPLE_NUMBER, LIMIT_SIZE , WORD_DICT_SIZE, TEST_SET_PROPORTION, MY_EMBEDDING_DIM, UNIT_COUNT, CNN_Encoder, RNN_Decoder, MY_OPTIMIZER, MY_LOSS_OBJECT, REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN, EPOCH_COUNT, feature_extraction_model, split  # hadie\n",
        "from timeit import default_timer as timer  # hadie\n",
        "import threading  # hadie\n",
        "\n",
        "import cv2\n",
        "import gc\n",
        "\n",
        "# from yolo import image_path_to_yolo_bounding_boxes  # hadie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyDj8DVL3LbI",
        "outputId": "c22b421d-fe5b-49f2-a045-9c427552fb74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=2.44s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "# Hong\n",
        "# 1. Load COCO training data\n",
        "\n",
        "PATH ='/content/drive/MyDrive/coco/train2014/'\n",
        "annotation_file = \"/content/drive/MyDrive/coco/annotations/captions_train2014.json\"\n",
        "\n",
        "coco = COCO(annotation_file)\n",
        "all_Ids = coco.getImgIds()\n",
        "input_ids = shuffle(all_Ids, random_state=0)\n",
        "\n",
        "num_examples = 20000\n",
        "input_ids = input_ids[:num_examples]\n",
        "\n",
        "input_img_name = []\n",
        "for i in range(len(input_ids)):\n",
        "    input_img_name.append(PATH+'COCO_train2014_' + '%012d' % (input_ids[i]) + '.jpg')\n",
        "\n",
        "### input_data = {img_name_train[i]: [cap_1, cap_2, ... cap_5]}\n",
        "input_data = {}\n",
        "for i in range(0,len(input_img_name)):\n",
        "    if input_img_name[i] not in input_data:\n",
        "        annIds = coco.getAnnIds(imgIds = input_ids[i])\n",
        "        anns = coco.loadAnns(annIds)\n",
        "        captions = []\n",
        "        for j in range(0,len(anns)):\n",
        "            a = '<start> '+ anns[j]['caption'] + ' <end>'\n",
        "            captions.append(a.lower())\n",
        "        input_data[input_img_name[i]] = captions\n",
        "\n",
        "# print(input_img_name[:5])\n",
        "# print(input_ids[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwnxYJtRa3Gm",
        "outputId": "5f456f8e-3eb5-429d-8e65-f61932de730a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-f7269b3131b1>:1: DeprecationWarning: Please use `argsort` from the `scipy.linalg` namespace, the `scipy.linalg.decomp` namespace is deprecated.\n",
            "  from scipy.linalg.decomp import argsort\n"
          ]
        }
      ],
      "source": [
        "from scipy.linalg.decomp import argsort\n",
        "from IPython.utils.text import indent\n",
        "\n",
        "net = cv2.dnn.readNet('yolov4.weights', 'yolov4.cfg')\n",
        "scale = 0.00392 #=1/255\n",
        "layer_names = ['conv_116','yolo_139','yolo_150','yolo_161'] # feature and 3 head layers\n",
        "\n",
        "def extract_yolo_features(filename):\n",
        "    image = cv2.imread(filename)\n",
        "    blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    layers = net.forward(layer_names)\n",
        "\n",
        "    return layers\n",
        "\n",
        "def extract_combined_feature(filename):\n",
        "    extracted_layers = extract_yolo_features(filename)\n",
        "\n",
        "    ####### OBJECTS ###########\n",
        "\n",
        "    # concat 3 output layers\n",
        "    out = np.concatenate((extracted_layers[1], extracted_layers[2], extracted_layers[3]), axis = 0)\n",
        "\n",
        "    ### NMS to get bounding boxes\n",
        "\n",
        "    image = cv2.imread(filename)\n",
        "    # Width = image.shape[1]\n",
        "    # Height = image.shape[0]\n",
        "\n",
        "    class_ids=[]\n",
        "    confidences=[]\n",
        "    p = []\n",
        "    boxes=[]\n",
        "    conf_threshold, nms_threshold = 0.5, 0.4\n",
        "\n",
        "    for detection in out:\n",
        "        scores = detection[5:]\n",
        "        class_id = np.argmax(scores)\n",
        "        confidence = scores[class_id] # prop of class\n",
        "        if confidence > conf_threshold:\n",
        "            p.append(detection[4])\n",
        "\n",
        "            class_ids.append(class_id)\n",
        "            confidences.append(float(confidence))\n",
        "            boxes.append([detection[0], detection[1], detection[2], detection[3]])\n",
        "\n",
        "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
        "    # print('indices: ', indices)\n",
        "\n",
        "    bounding_boxes=[]\n",
        "    important=[]\n",
        "    for i in range(len(list(indices))):\n",
        "        # boxes = [x, y, w, h, p, class_ids, important_factor]\n",
        "\n",
        "        ########## can takeover this 4 position values to save more [class_id, important_factor]\n",
        "        important_factor = boxes[indices[i]][2] * boxes[indices[i]][3] * confidences[indices[i]]\n",
        "        important.append(important_factor)\n",
        "\n",
        "        bbx = boxes[indices[i]].copy()\n",
        "        bbx.extend([p[indices[i]], class_ids[indices[i]], important_factor])\n",
        "\n",
        "        bounding_boxes.append(bbx)\n",
        "\n",
        "    ### sort bounding box to get the most important bounding box first\n",
        "\n",
        "    ranked = np.argsort(np.array(important))\n",
        "    largest_indices = ranked[::-1]\n",
        "\n",
        "    sorted_bbx=[]\n",
        "    for i in largest_indices:\n",
        "        b = bounding_boxes[i].copy()\n",
        "        sorted_bbx.append(b)\n",
        "    # print('sorted_bbx',sorted_bbx)\n",
        "\n",
        "    # flattening and padding to fit size of feature vector\n",
        "\n",
        "    ##### BACKBONE ##########\n",
        "\n",
        "    backbone_feature = extracted_layers[0] # conv_116: 1x512x13x13\n",
        "    # print('backbone_feature', backbone_feature.shape)\n",
        "\n",
        "    backbone_feature = backbone_feature.reshape(backbone_feature.shape[0] * backbone_feature.shape[1], -1) # 512x169\n",
        "    # print('backbone_feature.shape',backbone_feature.shape) # (512, 169)\n",
        "\n",
        "    features_shape = backbone_feature.shape[0] # 512\n",
        "    # print('features_shape', features_shape)\n",
        "\n",
        "    yolo_features = np.array(sorted_bbx).flatten()\n",
        "    # print('yolo_features.shape 1', yolo_features.shape)\n",
        "\n",
        "    # get the first num_of_box to fit the size of feature_shape\n",
        "    num_of_box = features_shape//7          # choose number of the most important boxes\n",
        "\n",
        "    yolo_features = yolo_features[:num_of_box*7]\n",
        "    yolo_features = np.pad(yolo_features, (0, features_shape - yolo_features.shape[0]), 'constant', constant_values=(0, 0)).astype(np.float32)\n",
        "    # print('yolo_features.shape 2', yolo_features.shape) # (512,)\n",
        "    yolo_features = yolo_features.reshape(yolo_features.shape[0], -1)\n",
        "    # print('yolo_features.shape 3', yolo_features.shape) # (512, 1)\n",
        "\n",
        "    feature = np.concatenate((backbone_feature, yolo_features), axis=1) #\n",
        "    feature = np.transpose(feature)\n",
        "    # print('combined feature shape', feature.shape) # (170, 512)\n",
        "\n",
        "    return feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D89sdlEObDPI",
        "outputId": "c1ba66f2-e9d8-426e-d66f-79e6bb33e457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------START OF EXECUTION-----------------------------\n",
            "extracting features (0) file(s)\n",
            "len of extracting 0\n",
            "\n",
            "finished extracting features\n"
          ]
        }
      ],
      "source": [
        "# extract combined feature for each image\n",
        "\n",
        "print(\"-----------------------------START OF EXECUTION-----------------------------\")  # hadie\n",
        "\n",
        "feature_dir = './3_Yolo4_RNN/Yolo4_conv116_npy_512_169'\n",
        "\n",
        "encode_train = [x for x in input_img_name if not os.path.exists(feature_dir + \"/\" + x[-31:-4] + \".npy\")]\n",
        "# features_shape = 2048\n",
        "print(\"extracting features (\" + str(len(encode_train)) + \") file(s)\")\n",
        "print('len of extracting',len (encode_train))\n",
        "\n",
        "if len(encode_train) > 0:\n",
        "    # print(encode_train)\n",
        "    # Feel free to change batch_size according to your system configuration\n",
        "    image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "    image_dataset = image_dataset.batch(16)\n",
        "\n",
        "    for paths in tqdm(image_dataset):\n",
        "        for path in paths:\n",
        "            fn = path.numpy().decode(\"utf-8\")\n",
        "            combined_feature = extract_combined_feature(fn)\n",
        "            np.save(feature_dir + \"/\" + fn[-31:-4], combined_feature)\n",
        "\n",
        "            del fn\n",
        "            del combined_feature\n",
        "            gc.collect()\n",
        "\n",
        "print(\"\\nfinished extracting features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct5C2UkjK4_-",
        "outputId": "230f7e52-3bfa-4413-c9d9-51bd66ece0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using the cashed tokenizer\n"
          ]
        }
      ],
      "source": [
        "\"\"\"## Preprocess and tokenize the captions\n",
        "\n",
        "* First, you'll tokenize the captions (for example, by splitting on spaces).\n",
        "This gives us a  vocabulary of all of the unique words in the data (for example, \"surfing\", \"football\", and so on).\n",
        "* Next, you'll limit the vocabulary size to the top 5,000 words (to save memory).\n",
        "You'll replace all other words with the token \"UNK\" (unknown).\n",
        "* You then create word-to-index and index-to-word mappings.\n",
        "* Finally, you pad all sequences to be the same length as the longest one.\n",
        "\"\"\"\n",
        "# Hong\n",
        "def dict_to_list(train_descriptions):\n",
        "    all_desc = []\n",
        "    for key in train_descriptions.keys():\n",
        "        [all_desc.append(d) for d in train_descriptions[key]]\n",
        "    return all_desc\n",
        "\n",
        "# Find the maximum length of any caption in our dataset\n",
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "# Choose the top 5000 words from the vocabulary\n",
        "top_k = WORD_DICT_SIZE  # hadie\n",
        "\n",
        "##################################################\n",
        "REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN = False\n",
        "\n",
        "input_captions = dict_to_list(input_data) # Hong\n",
        "# print('input_captions',input_captions)\n",
        "\n",
        "if not REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:  # hadie\n",
        "    print(\"using the cashed tokenizer\")  # hadie\n",
        "    # loading the tokenizer # hadie\n",
        "    with open(\"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/tokenizer.pickle\", 'rb') as handle:  # hadie\n",
        "        tokenizer = pickle.load(handle)  # hadie\n",
        "else:  # hadie\n",
        "    print(\"tokenizing and padding captions\")  # hadie\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                      oov_token=\"<unk>\",\n",
        "                                                      filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "    # input_captions = dict_to_list(input_data) # Hong\n",
        "    tokenizer.fit_on_texts(input_captions)\n",
        "\n",
        "    # train_seqs = tokenizer.texts_to_sequences(input_captions)  # 777 maybe this line needs removal\n",
        "    tokenizer.word_index['<pad>'] = 0\n",
        "    tokenizer.index_word[0] = '<pad>'\n",
        "    # saving the tokenizer to disk # hadie\n",
        "    with open(\"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/tokenizer.pickle\", 'wb') as handle:  # hadie\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)  # hadie\n",
        "\n",
        "# Create the tokenized vectors\n",
        "# input_seqs = tokenizer.texts_to_sequences(input_captions)\n",
        "\n",
        "# # Pad each vector to the max_length of the captions\n",
        "# # If you do not provide a max_length value, pad_sequences calculates it automatically\n",
        "# cap_vector = tf.keras.preprocessing.sequence.pad_sequences(input_seqs, padding='post')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSrBmdN8Egbm"
      },
      "outputs": [],
      "source": [
        "# Create training and validation sets using an 90-10 split\n",
        "\n",
        "PROP = 0.1\n",
        "\n",
        "if PROP == 0:\n",
        "    image_id_train = input_ids\n",
        "    image_id_val = []\n",
        "    img_name_train = input_img_name\n",
        "    img_name_val = []\n",
        "    # cap_train = cap_vector\n",
        "    # cap_val = []\n",
        "elif PROP == 1:\n",
        "    image_id_train = []\n",
        "    image_id_val = input_ids\n",
        "    img_name_train = []\n",
        "    img_name_val = input_img_name\n",
        "    # cap_train = []\n",
        "    # cap_val = cap_vector\n",
        "else:\n",
        "    image_id_train, image_id_val, img_name_train_2, img_name_val_2 = train_test_split(\n",
        "                                                                    input_ids,  # hadie\n",
        "                                                                    input_img_name,\n",
        "                                                                    test_size=PROP,  # hadie\n",
        "                                                                    random_state=0)\n",
        "\n",
        "cap_train_list = []\n",
        "cap_val_list = []\n",
        "for i in range(len(img_name_train_2)):\n",
        "    for j in range(len(input_data[img_name_train_2[i]])):\n",
        "        cap_train_list.append(input_data[img_name_train_2[i]][j])\n",
        "for i in range(len(img_name_val_2)):\n",
        "    for j in range(len(input_data[img_name_val_2[i]])):\n",
        "        cap_val_list.append(input_data[img_name_val_2[i]][j])\n",
        "\n",
        "train_seqs = tokenizer.texts_to_sequences(cap_train_list)\n",
        "val_seqs = tokenizer.texts_to_sequences(cap_val_list)\n",
        "cap_train = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
        "cap_val = tf.keras.preprocessing.sequence.pad_sequences(val_seqs, padding='post')\n",
        "\n",
        "# Hong\n",
        "def dict_to_list_2(dict_data, list_key):\n",
        "    all_img_name = []\n",
        "    for i in range(len(list_key)):\n",
        "        if list_key[i] in dict_data:\n",
        "            for j in range(len(dict_data[list_key[i]])):\n",
        "                all_img_name.append(list_key[i])\n",
        "    return all_img_name\n",
        "\n",
        "img_name_train = dict_to_list_2(input_data, img_name_train_2)\n",
        "img_name_val = dict_to_list_2(input_data, img_name_val_2)\n",
        "\n",
        "# print(img_name_train)\n",
        "# print(cap_train_list)\n",
        "\n",
        "# print(input_ids)\n",
        "# print(input_img_name)\n",
        "# print(cap_train)\n",
        "# print(\"len(img_name_train) = \", len(img_name_train), \", len(cap_train) = \", len(cap_train), \", len(img_name_val) = \", len(img_name_val), \", len(cap_val) = \", len(cap_val))  # hadie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-mXbKtalD6t",
        "outputId": "09fb3d9f-ea46-4688-9d06-2940099777c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished tokenizing and padding captions\n"
          ]
        }
      ],
      "source": [
        "if not REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:  # hadie\n",
        "    file = \"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/max_length.txt\"  # hadie\n",
        "    with open(file, 'r') as filetoread:  # hadie\n",
        "        max_length = int(filetoread.readline())  # hadie\n",
        "else:  # hadie\n",
        "    # Calculates the max_length, which is used to store the attention weights\n",
        "    max_length = calc_max_length(train_seqs)\n",
        "\n",
        "    # file = \"trained_model_\" + feature_extraction_model + \"/max_length.txt\"  # hadie\n",
        "    file = \"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/max_length.txt\"  # hadie\n",
        "    with open(file, 'w') as filetowrite:  # hadie\n",
        "        filetowrite.write(str(max_length))  # write the maximum length to disk # hadie\n",
        "\n",
        "print(\"finished tokenizing and padding captions\")  # hadie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TksHgKSxabOY",
        "outputId": "add321df-5773-4e14-bcc6-987bc91a28e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_PrefetchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.int32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "\"\"\"## Create a tf.data dataset for training\n",
        "\n",
        "Our images and captions are ready! Next, let's create a tf.data dataset to use for training our model.\n",
        "\"\"\"\n",
        "\n",
        "# Feel free to change these parameters according to your system's configuration\n",
        "\n",
        "BATCH_SIZE = 8*5  # 64\n",
        "# BATCH_SIZE =16\n",
        "# BUFFER_SIZE = BATCH_SIZE*8\n",
        "embedding_dim = MY_EMBEDDING_DIM  # hadie\n",
        "units = UNIT_COUNT  # hadie\n",
        "vocab_size = top_k + 1\n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "# the definition of features_shape = 2048 was moved up\n",
        "# the attention_features variable has been moved to a separate file by hadie\n",
        "\n",
        "\n",
        "# Load the numpy files\n",
        "def map_func(img_name, cap):\n",
        "    img_tensor = np.load(feature_dir + '/' + img_name.decode('utf-8')[-31:-4] + '.npy')\n",
        "    return img_tensor, cap\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "dataset = dataset.shuffle(BATCH_SIZE*8)\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset = dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfAop6RnLlqC",
        "outputId": "993055d4-d9d9-403c-ae30-4d94a4efb8e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_epoch 10\n"
          ]
        }
      ],
      "source": [
        "\"\"\"## Model\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# the model has been moved to model.py by hadie\n",
        "\n",
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "optimizer = MY_OPTIMIZER  # hadie\n",
        "loss_object = MY_LOSS_OBJECT  # hadie\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "\n",
        "\"\"\"## Checkpoint\"\"\"\n",
        "\n",
        "checkpoint_path = \"./3_Yolo4_RNN/checkpoint\"\n",
        "\n",
        "if REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:\n",
        "    try:\n",
        "        for filename in os.listdir(checkpoint_path):\n",
        "            print(\"deleting \" + checkpoint_path + \"/\" + filename)\n",
        "            os.unlink(checkpoint_path + \"/\" + filename)\n",
        "    except Exception as e:\n",
        "        print('Failed to delete %s. Reason: %s' % (checkpoint_path + \"/\" + filename, e))\n",
        "    # remove the saved model too\n",
        "    if os.path.exists(\"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/my_model.index\"):\n",
        "        print(\"deleting ./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/my_model.index\")\n",
        "        os.unlink(\"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/my_model.index\")\n",
        "    if os.path.exists(\"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/checkpoint\"):\n",
        "        print(\"deleting ./3_Yolo4_RNN/trained_model_Yoloconv116_RNN_conv116/checkpoint\")\n",
        "        os.unlink(\"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/checkpoint\")\n",
        "    if os.path.exists(\"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/my_model.data-00000-of-00001\"):\n",
        "        print(\"deleting ./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/my_model.data-00000-of-00001\")\n",
        "        os.unlink(\"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/my_model.data-00000-of-00001\")\n",
        "    if os.path.exists(\"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/learning_curve.png\"):\n",
        "        print(\"deleting ./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/learning_curve.png\")\n",
        "        os.unlink(\"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/learning_curve.png\")\n",
        "\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "    print('start_epoch', start_epoch)\n",
        "    # restoring the latest checkpoint in checkpoint_path\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMZnaCxvR4ah",
        "outputId": "b52fed7a-5f28-4a1c-eeaa-3f338ba1d52b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A trained model has been found. Loading it from disk..\n"
          ]
        }
      ],
      "source": [
        "\"\"\"## Training\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# adding this in a separate cell because if you run the training cell\n",
        "# many times, the loss_plot array will be reset\n",
        "loss_plot = []\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "\n",
        "    # initializing the hidden state for each batch\n",
        "    # because the captions are not related from image to image\n",
        "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(img_tensor)\n",
        "        # print('encoder', encoder.count_params())\n",
        "\n",
        "        for i in range(1, target.shape[1]):\n",
        "            # passing the features through the decoder\n",
        "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "            # print('decoder', decoder.count_params())\n",
        "\n",
        "            loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    del hidden\n",
        "    del dec_input\n",
        "    del features\n",
        "    del predictions\n",
        "    del trainable_variables\n",
        "    gc.collect()\n",
        "\n",
        "    return loss, total_loss\n",
        "\n",
        "\n",
        "# EPOCHS = EPOCH_COUNT  # hadie\n",
        "EPOCHS =10\n",
        "\n",
        "if not os.path.exists(\"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/my_model.index\"):\n",
        "    print(\"training..\")  # hadie\n",
        "\n",
        "    t1 = time.time()\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        start = time.time()\n",
        "        total_loss = 0\n",
        "\n",
        "        for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "            # print('img_tensor.shape', img_tensor.shape)\n",
        "            # print('target', target.shape)\n",
        "            batch_loss, t_loss = train_step(img_tensor, target)\n",
        "            total_loss += t_loss\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "                  epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
        "        # storing the epoch end loss value to plot later\n",
        "        loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "        # if epoch % 5 == 0:\n",
        "        ckpt_manager.save()\n",
        "\n",
        "        print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
        "                                             total_loss / num_steps))\n",
        "        print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "    t2 = time.time()\n",
        "\n",
        "    # save model to disk # hadie\n",
        "    decoder.save_weights(\"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/my_model\" , save_format=\"tf\")  # hadie\n",
        "    print('time of training: ', t2-t1)\n",
        "else:  # hadie\n",
        "    print(\"A trained model has been found. Loading it from disk..\")  # hadie\n",
        "    # Load the previously saved weights # hadie\n",
        "    decoder.load_weights(\"./3_Yolo4_RNN/trained_model_Yolo_RNN_conv116/my_model\")  # hadie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "r_8xSHL6z1wE",
        "outputId": "305dde0d-a967-4cc6-d5c2-432a229c1db0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-17f67cb8cef0>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# print ('Real Caption:', real_caption)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-17f67cb8cef0>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_features_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# print(image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcombined_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_combined_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# print('combined_features.shape', combined_features.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_features\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# hadie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f7269b3131b1>\u001b[0m in \u001b[0;36mextract_combined_feature\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_combined_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mextracted_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_yolo_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m####### OBJECTS ###########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f7269b3131b1>\u001b[0m in \u001b[0;36mextract_yolo_features\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_yolo_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblobFromImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
          ]
        }
      ],
      "source": [
        "\"\"\"## Caption!\n",
        "\n",
        "\"\"\"\n",
        "attention_features_shape = 170\n",
        "\n",
        "def evaluate(image):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "    # print(image)\n",
        "    combined_features = extract_combined_feature(image)\n",
        "    # print('combined_features.shape', combined_features.shape)\n",
        "    features = encoder(combined_features)  # hadie\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1,)).numpy()\n",
        "\n",
        "        # predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        predicted_id = np.argmax(predictions.numpy())\n",
        "\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result),:]\n",
        "    return result, attention_plot\n",
        "\n",
        "\n",
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for l in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
        "        ax = fig.add_subplot(len_result // 2, len_result // 2, l + 1)\n",
        "        ax.set_title(result[l])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def list_to_dict(name_list, caption_list):\n",
        "    name_dict = {}\n",
        "    for i in range(len(name_list)):\n",
        "        if name_list[i] not in name_dict:\n",
        "            name_dict[name_list[i]] = [caption_list[i]]\n",
        "        else:\n",
        "            name_dict.get(name_list[i]).append(caption_list[i])\n",
        "            # print( name_dict.get(name_list[i]))\n",
        "    return name_dict\n",
        "\n",
        "\n",
        "# captions on the validation set\n",
        "val_dict = list_to_dict(img_name_val, cap_val)\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "for i in range(10):\n",
        "    # rid = np.random.randint(0, len(img_name_val))\n",
        "    image = img_name_val[i*5]\n",
        "\n",
        "    # real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "    captions = val_dict[image]\n",
        "\n",
        "    result, attention_plot = evaluate(image)\n",
        "\n",
        "    # print ('Real Caption:', real_caption)\n",
        "    print('Real captions:')\n",
        "    for j in range(len(captions)):\n",
        "        real_caption = ' '.join([tokenizer.index_word[i] for i in captions[j] if i not in [0]])\n",
        "        print(real_caption)\n",
        "\n",
        "    print ('Prediction Caption:', ' '.join(result))\n",
        "    # plot_attention(image, result, attention_plot)\n",
        "\n",
        "    print('img path', image)\n",
        "\n",
        "    img = cv2.imread(image)\n",
        "    cv2_imshow(img)\n",
        "\n",
        "# list_img = [\n",
        "#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000117179.jpg',\n",
        "#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000335744.jpg',\n",
        "#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000033435.jpg',\n",
        "#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000555896.jpg',\n",
        "#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000448069.jpg',\n",
        "#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000367171.jpg',\n",
        "#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000039758.jpg',\n",
        "#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000192145.jpg',\n",
        "#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000493483.jpg',\n",
        "#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000519404.jpg'\n",
        "\n",
        "# ]\n",
        "\n",
        "# for image in list_img:\n",
        "#     # real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "#     captions = val_dict[image]\n",
        "\n",
        "#     result, attention_plot = evaluate(image)\n",
        "\n",
        "#     # print ('Real Caption:', real_caption)\n",
        "#     print('Real captions:')\n",
        "#     for j in range(len(captions)):\n",
        "#         real_caption = ' '.join([tokenizer.index_word[i] for i in captions[j] if i not in [0]])\n",
        "#         print(real_caption)\n",
        "\n",
        "#     print ('Prediction Caption:', ' '.join(result))\n",
        "#     # plot_attention(image, result, attention_plot)\n",
        "\n",
        "#     print('img path', image)\n",
        "\n",
        "#     img = cv2.imread(image)\n",
        "#     cv2_imshow(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRVTZv2IkjHV",
        "outputId": "ac253a29-11e0-47dd-d1a5-0593a9d13796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "100%|██████████| 2000/2000 [31:32<00:00,  1.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "bleu-1 score: 0.6319255697261134\n",
            "bleu-2 score: 0.45408257378800776\n",
            "bleu-3 score: 0.35348939202602025\n",
            "bleu-4 score: 0.2139480076050468\n",
            "meteor score: 0.41103270000000053\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "def img_to_caption(img_name):\n",
        "    result, attention_plot = evaluate(img_name)\n",
        "    result = ' '.join(result).replace(\"<end>\", \"\").strip()\n",
        "    return result\n",
        "\n",
        "actual, predicted = [], []\n",
        "\n",
        "for img_name in tqdm(val_dict):\n",
        "\n",
        "    yhat = img_to_caption(img_name)\n",
        "    pred = yhat.split()\n",
        "\n",
        "    actu = []\n",
        "    captions = val_dict[img_name]\n",
        "    for i in range(len(captions)):\n",
        "        real_caption = ' '.join([tokenizer.index_word[i] for i in captions[i] if i not in [0,3,4]])\n",
        "        references = real_caption.split(' ')\n",
        "        actu.append(references)\n",
        "\n",
        "    actual.append(actu)\n",
        "    predicted.append(pred)\n",
        "\n",
        "    # print('actual', actual)\n",
        "    # print('predicted', predicted)\n",
        "\n",
        "bl1 = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
        "bl2 = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n",
        "bl3 = corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))\n",
        "bl4 = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "mt = 0\n",
        "for i in range(len(predicted)):\n",
        "    mt += round(meteor_score(actual[i], predicted[i]),4)\n",
        "mt = mt/len(predicted)\n",
        "\n",
        "print(\"\\nbleu-1 score:\", bl1)\n",
        "print(\"bleu-2 score:\", bl2)\n",
        "print(\"bleu-3 score:\", bl3)\n",
        "print(\"bleu-4 score:\", bl4)\n",
        "print(\"meteor score:\", mt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeYcsfVNoT0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43fa660e-1260-49b4-ddb7-e65960fc87fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time of predicting captions for 100 images: 59.2064745426178\n"
          ]
        }
      ],
      "source": [
        "# speed test\n",
        "\n",
        "# for random samples\n",
        "img_list = []\n",
        "for i in range(100):\n",
        "    rid = np.random.randint(0, len(img_name_val))\n",
        "    img_list.append(img_name_val[rid])\n",
        "\n",
        "t1 = time.time()\n",
        "for image in img_list:\n",
        "    result, attention_plot = evaluate(image)\n",
        "    predict = ' '.join(result)\n",
        "\n",
        "t2 = time.time()\n",
        "print('time of predicting captions for 100 images:', t2-t1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxtTsgyO70G9"
      },
      "outputs": [],
      "source": [
        "# # speed test => moved to local test\n",
        "\n",
        "# file = open('./save/img_name_val_bk.txt', 'r')\n",
        "# txt = file.read()\n",
        "# file.close()\n",
        "\n",
        "# test_imgs1 = txt[1:-1].split(\", \")\n",
        "# test_imgs = []\n",
        "# num_test_file = 5\n",
        "# for i in range(num_test_file):\n",
        "#     test_imgs.append(test_imgs1[i][1:-1])\n",
        "\n",
        "# # ['/content/drive/MyDrive/coco/train2014/COCO_train2014_000000151458.jpg',\n",
        "# # '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000419159.jpg',\n",
        "# # '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000239584.jpg',\n",
        "# # '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000521266.jpg',\n",
        "# # '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000337415.jpg']\n",
        "\n",
        "# time_extract = 0\n",
        "# time_predict = 0\n",
        "\n",
        "# def predict_caption(image):\n",
        "\n",
        "#     time_start_extract = time.time()\n",
        "#     combined_features = extract_combined_feature(image)\n",
        "#     t = time.time() - time_start_extract\n",
        "\n",
        "#     features = encoder(combined_features)\n",
        "\n",
        "#     hidden = decoder.reset_state(batch_size=1)\n",
        "#     dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "#     result = []\n",
        "#     for i in range(max_length):\n",
        "#         predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "\n",
        "#         predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "\n",
        "#         # hong\n",
        "#         while predicted_id > len(tokenizer.index_word)-1:\n",
        "#             predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "#         # hong end\n",
        "\n",
        "#         result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "#         if tokenizer.index_word[predicted_id] == '<end>':\n",
        "#             # print('early')\n",
        "#             return result, t\n",
        "\n",
        "#         dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "#     return result, t\n",
        "\n",
        "\n",
        "# # captions on the validation set\n",
        "# n = len(test_imgs)\n",
        "# for i in range(n):\n",
        "#     image = test_imgs[i]\n",
        "\n",
        "#     time_start_predict = time.time()\n",
        "#     result, t = predict_caption(image)\n",
        "#     time_predict += time.time() - time_start_predict\n",
        "#     time_extract += t\n",
        "\n",
        "#     print ('Prediction Caption:', ' '.join(result))\n",
        "\n",
        "#     # print('img path', image)\n",
        "#     # import cv2\n",
        "#     # from google.colab.patches import cv2_imshow\n",
        "#     # img = cv2.imread(image)\n",
        "#     # cv2_imshow(img)\n",
        "\n",
        "# time_extract /= n\n",
        "# time_predict /= n\n",
        "# print('average time_extract by Yolo_RNN', time_extract)\n",
        "# print('average time_predict by Yolo_RNN', time_predict)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNgIZd5vPt5FbpfikQ/hytX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}