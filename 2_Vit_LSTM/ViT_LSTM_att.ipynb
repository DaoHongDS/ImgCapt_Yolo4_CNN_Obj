{"cells":[{"cell_type":"markdown","metadata":{"id":"r_SZJWXF_Uqh"},"source":["ViT_LSTM_att"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17972,"status":"ok","timestamp":1702831244692,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"8kcoqgZVXw9k","outputId":"9ce370f5-3a40-4c2a-b0a9-315bb969502d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":619,"status":"ok","timestamp":1701793309433,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"SEH9vcz_X44v","outputId":"2ae1a5bf-100b-41a7-df66-901cb230390a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Dec  5 16:21:01 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7747,"status":"ok","timestamp":1701793325031,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"oresTuTGX7S3","outputId":"008f1711-2cac-449d-bb48-8b3f72c3be2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["'/content/drive/MyDrive/coco/pycocotools' -> '/content/pycocotools'\n","'/content/drive/MyDrive/coco/pycocotools/_mask.pyx' -> '/content/pycocotools/_mask.pyx'\n","'/content/drive/MyDrive/coco/pycocotools/__init__.py' -> '/content/pycocotools/__init__.py'\n","'/content/drive/MyDrive/coco/pycocotools/_mask.c' -> '/content/pycocotools/_mask.c'\n","'/content/drive/MyDrive/coco/pycocotools/_mask.cp39-win_amd64.pyd' -> '/content/pycocotools/_mask.cp39-win_amd64.pyd'\n","'/content/drive/MyDrive/coco/pycocotools/mask.py' -> '/content/pycocotools/mask.py'\n","'/content/drive/MyDrive/coco/pycocotools/coco.py' -> '/content/pycocotools/coco.py'\n","'/content/drive/MyDrive/coco/pycocotools/__pycache__' -> '/content/pycocotools/__pycache__'\n","'/content/drive/MyDrive/coco/pycocotools/__pycache__/coco.cpython-39.pyc' -> '/content/pycocotools/__pycache__/coco.cpython-39.pyc'\n","'/content/drive/MyDrive/coco/pycocotools/__pycache__/__init__.cpython-39.pyc' -> '/content/pycocotools/__pycache__/__init__.cpython-39.pyc'\n","'/content/drive/MyDrive/coco/pycocotools/__pycache__/mask.cpython-39.pyc' -> '/content/pycocotools/__pycache__/mask.cpython-39.pyc'\n","'/content/drive/MyDrive/coco/pycocotools/cocoeval.py' -> '/content/pycocotools/cocoeval.py'\n"]}],"source":["!cp -av '/content/drive/MyDrive/coco/pycocotools' '/content/'\n","\n","import os\n","os.chdir('/content/drive/MyDrive/YOLOv4_CNN_RNN')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCOYixMmY-CG"},"outputs":[],"source":["import tensorflow as tf\n","tf.get_logger().setLevel('ERROR')  # hadie\n","# You'll generate plots of attention in order to see which parts of an image\n","# our model focuses on  during captioning\n","\n","import matplotlib.pyplot as plt\n","\n","from pycocotools.coco import COCO\n","\n","# Scikit-learn includes many helpful utilities\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","import re\n","import numpy as np\n","import os\n","import time\n","import json\n","from glob import glob\n","from PIL import Image\n","import pickle\n","\n","from tqdm import tqdm  # hadie\n","from termcolor import colored  # hadie\n","from builtins import len  # hadie\n","import datetime  # hadie\n","\n","from model import DATASET, EXAMPLE_NUMBER, LIMIT_SIZE , WORD_DICT_SIZE, TEST_SET_PROPORTION, MY_EMBEDDING_DIM, UNIT_COUNT, CNN_Encoder, RNN_Decoder, MY_OPTIMIZER, MY_LOSS_OBJECT, REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN, EPOCH_COUNT, feature_extraction_model, split  # hadie\n","from timeit import default_timer as timer  # hadie\n","import threading  # hadie\n","\n","import cv2\n","from transformers import AutoImageProcessor, ViTModel\n","import gc\n","\n","# from yolo import image_path_to_yolo_bounding_boxes  # hadie"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4040,"status":"ok","timestamp":1701793347390,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"VyDj8DVL3LbI","outputId":"5e8454f9-9fda-474d-e3bf-4e1d7c9f579d"},"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=3.08s)\n","creating index...\n","index created!\n"]}],"source":["# Hong\n","# 1. Load COCO training data\n","\n","PATH ='/content/drive/MyDrive/coco/train2014/'\n","annotation_file = \"/content/drive/MyDrive/coco/annotations/captions_train2014.json\"\n","\n","coco = COCO(annotation_file)\n","all_Ids = coco.getImgIds()\n","input_ids = shuffle(all_Ids, random_state=0)\n","\n","num_examples = 20000\n","input_ids = input_ids[:num_examples]\n","\n","input_img_name = []\n","for i in range(len(input_ids)):\n","    input_img_name.append(PATH+'COCO_train2014_' + '%012d' % (input_ids[i]) + '.jpg')\n","\n","### input_data = {img_name_train[i]: [cap_1, cap_2, ... cap_5]}\n","input_data = {}\n","for i in range(0,len(input_img_name)):\n","    if input_img_name[i] not in input_data:\n","        annIds = coco.getAnnIds(imgIds = input_ids[i])\n","        anns = coco.loadAnns(annIds)\n","        captions = []\n","        for j in range(0,len(anns)):\n","            a = '<start> '+ anns[j]['caption'] + ' <end>'\n","            captions.append(a.lower())\n","        input_data[input_img_name[i]] = captions\n","\n","# print(input_img_name[:5])\n","# print(input_ids[:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwnxYJtRa3Gm"},"outputs":[],"source":["############## FEATURES WITH MANUAL PREPROCESS ###############\n","\n","# import torch\n","# import torchvision.transforms as transforms\n","\n","# # image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n","# vit_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","# for param in vit_encoder.parameters():\n","#     param.requires_grad = False\n","\n","# total_params = sum(p.numel() for p in vit_encoder.parameters())\n","# print('Params of ViT:', total_params)\n","\n","# IMAGE_SIZE = (224, 224)\n","# transform = transforms.Compose([\n","#     transforms.PILToTensor()\n","# ])\n","\n","# def extract_vit_feature(filename):\n","#     image = Image.open(filename).convert(\"RGB\")\n","#     image = image.resize(IMAGE_SIZE)\n","\n","#     img_pytorch_tensor = transform(image)\n","#     img_pytorch_tensor = torch.unsqueeze(img_pytorch_tensor, 0)\n","\n","#     pt_tensor_features = vit_encoder(img_pytorch_tensor).last_hidden_state\n","\n","#     np_tensor_features = pt_tensor_features.numpy() # 1x197x768\n","#     features = np_tensor_features.reshape(-1, np_tensor_features.shape[2]) # 197x768\n","\n","#     del image\n","#     del img_pytorch_tensor\n","#     del np_tensor_features\n","#     gc.collect()\n","\n","#     return features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131,"referenced_widgets":["620cfda95720483fb9a6083328226a22","7560884585fb4233b17c8aacc902c2b3","0c1352f6614941a39a1cc3231968b0f3","742ac464ead34febac1332e6ad7f19cc","a5a8d04c15f44cb9a06fa324e620dc7c","950971fee9b94963a0c5a59ec13322a6","febec9b7d2624c2ca813053376710e88","0b76370f6b7145a8b445f67d76bc9551","6000fef7c85b40c4a5bdc2c3671e1d77","0fe2fc8692df4fbea60e7f8f5bc1f272","afebf6384f0b47d8b590832c9c40bb90","7beff1706b2d400ab6a33ae3588ec536","805526378c4f4c8b92d9d6fc51a78e9c","c68c58b489b6434e9363827abf9b0819","ef04257b625c455cbb6f022109ac0b7a","3acbe04345ca4f839bbe50d1f11d57b6","4b6fcaa0d7af4a0999ca9c42d5efd98b","c3e734ae148c410388fa40bb4c12d84e","5cd24d363817444dba956e8060687015","65c4fc7971f04b07b6ae62018facac78","36d99f10912d4a6987c9fd99bc4fc06c","510ef5aee90449ac9d65875536f73bd8","1c125eda82e944a79d4c62338ea76ea9","59d322b641a9422b88e53dbe85081877","9e99f84bbe7f409eb610af7fed2b4f7b","45541e26638b4f0a8f368704ff8c4c32","a318b2d7fff1464db6772e3246c21e3f","66dd3853320049f7a76d020e434dd305","7a38378a2678497d92f6ca0ceda24203","ce7eecd7972c41a58cc039d7a2b5312f","9732c270980f41ce952fe38907f60791","4be5d015c59743dab87b55847b3c59c9","57dca04963004400bee0003c75c4e701"]},"executionInfo":{"elapsed":5545,"status":"ok","timestamp":1701793357139,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"0GGZajXv2Gpm","outputId":"a012f4e2-b7a3-4a0f-84b0-06e1307af921"},"outputs":[{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"620cfda95720483fb9a6083328226a22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7beff1706b2d400ab6a33ae3588ec536"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/346M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c125eda82e944a79d4c62338ea76ea9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Params of ViT: 86389248\n"]}],"source":["############################################################\n","############## FEATURES WITH AUTO PREPROCESS ###############\n","############################################################\n","\n","import torch\n","import torchvision.transforms as transforms\n","\n","image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n","vit_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","for param in vit_encoder.parameters():\n","    param.requires_grad = False\n","\n","total_params = sum(p.numel() for p in vit_encoder.parameters())\n","print('Params of ViT:', total_params)\n","\n","def extract_vit_feature(filename):\n","    image = Image.open(filename).convert('RGB')\n","    inputs = image_processor(image, return_tensors=\"pt\")\n","\n","    with torch.no_grad():\n","        outputs = vit_encoder(**inputs)\n","\n","    pt_tensor_features = outputs.last_hidden_state\n","\n","    np_tensor_features = pt_tensor_features.numpy() # 1x197x768\n","    features = np_tensor_features.reshape(-1, np_tensor_features.shape[2]) # 197x768\n","\n","    del image\n","    del inputs\n","    del outputs\n","    del pt_tensor_features\n","    del np_tensor_features\n","    gc.collect()\n","\n","    return features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D89sdlEObDPI","executionInfo":{"status":"ok","timestamp":1701793763257,"user_tz":-420,"elapsed":2206,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"}},"outputId":"01a56c56-80ab-4d4d-b1fd-ae1eb206ea83"},"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------START OF EXECUTION-----------------------------\n","extracting features (0) file(s)\n","len of extracting 0\n","\n","finished extracting features\n"]}],"source":["# extract combined feature for each image\n","\n","print(\"-----------------------------START OF EXECUTION-----------------------------\")  # hadie\n","\n","feature_dir = './2_Vit_LSTM/ViT_npy_preprocess'\n","\n","encode_train = [x for x in input_img_name if not os.path.exists(feature_dir + \"/\" + x[-31:-4] + \".npy\")]\n","# features_shape = 2048\n","print(\"extracting features (\" + str(len(encode_train)) + \") file(s)\")\n","print('len of extracting',len (encode_train))\n","\n","if len(encode_train) > 0:\n","    # print(encode_train)\n","    # Feel free to change batch_size according to your system configuration\n","    image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n","    image_dataset = image_dataset.batch(16)\n","\n","    for paths in tqdm(image_dataset):\n","        for path in paths:\n","            fn = path.numpy().decode(\"utf-8\")\n","            combined_feature = extract_vit_feature(fn)\n","            np.save(feature_dir + \"/\" + fn[-31:-4], combined_feature)\n","\n","            del fn\n","            del combined_feature\n","            gc.collect()\n","\n","print(\"\\nfinished extracting features\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3467,"status":"ok","timestamp":1701793791442,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"Ct5C2UkjK4_-","outputId":"d995d381-84e2-4000-99eb-63f6b02e22fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["tokenizing and padding captions\n"]}],"source":["\"\"\"## Preprocess and tokenize the captions\n","\n","* First, you'll tokenize the captions (for example, by splitting on spaces).\n","This gives us a  vocabulary of all of the unique words in the data (for example, \"surfing\", \"football\", and so on).\n","* Next, you'll limit the vocabulary size to the top 5,000 words (to save memory).\n","You'll replace all other words with the token \"UNK\" (unknown).\n","* You then create word-to-index and index-to-word mappings.\n","* Finally, you pad all sequences to be the same length as the longest one.\n","\"\"\"\n","# Hong\n","def dict_to_list(train_descriptions):\n","    all_desc = []\n","    for key in train_descriptions.keys():\n","        [all_desc.append(d) for d in train_descriptions[key]]\n","    return all_desc\n","\n","# Find the maximum length of any caption in our dataset\n","def calc_max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","\n","# Choose the top 5000 words from the vocabulary\n","top_k = WORD_DICT_SIZE  # hadie\n","\n","##################################################\n","REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN = False\n","\n","input_captions = dict_to_list(input_data) # Hong\n","# print('input_captions',input_captions)\n","\n","if not REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:  # hadie\n","    print(\"using the cashed tokenizer\")  # hadie\n","    # loading the tokenizer # hadie\n","    with open(\"./2_Vit_LSTM/model_trained_ViT_LSTM_att/tokenizer.pickle\", 'rb') as handle:  # hadie\n","        tokenizer = pickle.load(handle)  # hadie\n","else:  # hadie\n","    print(\"tokenizing and padding captions\")  # hadie\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n","                                                      oov_token=\"<unk>\",\n","                                                      filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n","    # input_captions = dict_to_list(input_data) # Hong\n","    tokenizer.fit_on_texts(input_captions)\n","\n","    # train_seqs = tokenizer.texts_to_sequences(input_captions)  # 777 maybe this line needs removal\n","    tokenizer.word_index['<pad>'] = 0\n","    tokenizer.index_word[0] = '<pad>'\n","    # saving the tokenizer to disk # hadie\n","    with open(\"./2_Vit_LSTM/model_trained_ViT_LSTM_att/tokenizer.pickle\", 'wb') as handle:  # hadie\n","        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)  # hadie\n","\n","# Create the tokenized vectors\n","# input_seqs = tokenizer.texts_to_sequences(input_captions)\n","\n","# # Pad each vector to the max_length of the captions\n","# # If you do not provide a max_length value, pad_sequences calculates it automatically\n","# cap_vector = tf.keras.preprocessing.sequence.pad_sequences(input_seqs, padding='post')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSrBmdN8Egbm"},"outputs":[],"source":["# Create training and validation sets using an 90-10 split\n","\n","PROP = 0.1\n","\n","if PROP == 0:\n","    image_id_train = input_ids\n","    image_id_val = []\n","    img_name_train = input_img_name\n","    img_name_val = []\n","    # cap_train = cap_vector\n","    # cap_val = []\n","elif PROP == 1:\n","    image_id_train = []\n","    image_id_val = input_ids\n","    img_name_train = []\n","    img_name_val = input_img_name\n","    # cap_train = []\n","    # cap_val = cap_vector\n","else:\n","    image_id_train, image_id_val, img_name_train_2, img_name_val_2 = train_test_split(\n","                                                                    input_ids,  # hadie\n","                                                                    input_img_name,\n","                                                                    test_size=PROP,  # hadie\n","                                                                    random_state=0)\n","\n","cap_train_list = []\n","cap_val_list = []\n","for i in range(len(img_name_train_2)):\n","    for j in range(len(input_data[img_name_train_2[i]])):\n","        cap_train_list.append(input_data[img_name_train_2[i]][j])\n","for i in range(len(img_name_val_2)):\n","    for j in range(len(input_data[img_name_val_2[i]])):\n","        cap_val_list.append(input_data[img_name_val_2[i]][j])\n","\n","train_seqs = tokenizer.texts_to_sequences(cap_train_list)\n","val_seqs = tokenizer.texts_to_sequences(cap_val_list)\n","cap_train = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n","cap_val = tf.keras.preprocessing.sequence.pad_sequences(val_seqs, padding='post')\n","\n","# Hong\n","def dict_to_list_2(dict_data, list_key):\n","    all_img_name = []\n","    for i in range(len(list_key)):\n","        if list_key[i] in dict_data:\n","            for j in range(len(dict_data[list_key[i]])):\n","                all_img_name.append(list_key[i])\n","    return all_img_name\n","\n","img_name_train = dict_to_list_2(input_data, img_name_train_2)\n","img_name_val = dict_to_list_2(input_data, img_name_val_2)\n","\n","# print(img_name_train)\n","# print(cap_train_list)\n","\n","# print(input_ids)\n","# print(input_img_name)\n","# print(cap_train)\n","# print(\"len(img_name_train) = \", len(img_name_train), \", len(cap_train) = \", len(cap_train), \", len(img_name_val) = \", len(img_name_val), \", len(cap_val) = \", len(cap_val))  # hadie"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1133,"status":"ok","timestamp":1701793800551,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"J-mXbKtalD6t","outputId":"cf073966-d636-48c1-9a8c-91fa4ad5e750"},"outputs":[{"output_type":"stream","name":"stdout","text":["finished tokenizing and padding captions\n"]}],"source":["if not REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:  # hadie\n","    file = \"./2_Vit_LSTM/model_trained_ViT_LSTM_att/max_length.txt\"  # hadie\n","    with open(file, 'r') as filetoread:  # hadie\n","        max_length = int(filetoread.readline())  # hadie\n","else:  # hadie\n","    # Calculates the max_length, which is used to store the attention weights\n","    max_length = calc_max_length(train_seqs)\n","\n","    # file = \"trained_model_\" + feature_extraction_model + \"/max_length.txt\"  # hadie\n","    file = \"./2_Vit_LSTM/model_trained_ViT_LSTM_att/max_length.txt\"  # hadie\n","    with open(file, 'w') as filetowrite:  # hadie\n","        filetowrite.write(str(max_length))  # write the maximum length to disk # hadie\n","\n","print(\"finished tokenizing and padding captions\")  # hadie"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":481,"status":"ok","timestamp":1701793803924,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"TksHgKSxabOY","outputId":"d4934e58-2a61-4189-ecb7-3b2aa9a7ccfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["<_PrefetchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.int32, name=None))>\n"]}],"source":["\"\"\"## Create a tf.data dataset for training\n","\n","Our images and captions are ready! Next, let's create a tf.data dataset to use for training our model.\n","\"\"\"\n","\n","# Feel free to change these parameters according to your system's configuration\n","\n","BATCH_SIZE = 8*5  # 64\n","# BATCH_SIZE =16\n","# BUFFER_SIZE = BATCH_SIZE*8\n","embedding_dim = MY_EMBEDDING_DIM  # hadie\n","units = UNIT_COUNT  # hadie\n","vocab_size = top_k + 1\n","num_steps = len(img_name_train) // BATCH_SIZE\n","# Shape of the vector extracted from InceptionV3 is (64, 2048)\n","# These two variables represent that vector shape\n","# the definition of features_shape = 2048 was moved up\n","# the attention_features variable has been moved to a separate file by hadie\n","\n","\n","# Load the numpy files\n","def map_func(img_name, cap):\n","    img_tensor = np.load(feature_dir + '/' + img_name.decode('utf-8')[-31:-4] + '.npy')\n","    return img_tensor, cap\n","\n","dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n","dataset = dataset.shuffle(BATCH_SIZE*8)\n","\n","# Use map to load the numpy files in parallel\n","dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),\n","                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","dataset = dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","print(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SfAop6RnLlqC"},"outputs":[],"source":["\"\"\"## Model\n","\n","\"\"\"\n","\n","# the model has been moved to model.py by hadie\n","\n","encoder = CNN_Encoder(embedding_dim)\n","decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n","\n","optimizer = MY_OPTIMIZER  # hadie\n","loss_object = MY_LOSS_OBJECT  # hadie\n","\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)\n","\n","\n","\"\"\"## Checkpoint\"\"\"\n","\n","checkpoint_path = \"./2_Vit_LSTM/checkpoint\"\n","\n","if REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:\n","    try:\n","        for filename in os.listdir(checkpoint_path):\n","            print(\"deleting \" + checkpoint_path + \"/\" + filename)\n","            os.unlink(checkpoint_path + \"/\" + filename)\n","    except Exception as e:\n","        print('Failed to delete %s. Reason: %s' % (checkpoint_path + \"/\" + filename, e))\n","    # remove the saved model too\n","    if os.path.exists(\"./2_Vit_LSTM/model_trained_ViT_LSTM_att/my_model.index\"):\n","        print(\"deleting ./2_Vit_LSTM/model_trained_ViT_LSTM_att/my_model.index\")\n","        os.unlink(\"./2_Vit_LSTM/model_trained_ViT_LSTM_att/my_model.index\")\n","    if os.path.exists(\"./2_Vit_LSTM/model_trained_ViT_LSTM_att/checkpoint\"):\n","        print(\"deleting ./2_Vit_LSTM/model_trained_ViT_LSTM_att/checkpoint\")\n","        os.unlink(\"./2_Vit_LSTM/model_trained_ViT_LSTM_att/checkpoint\")\n","    if os.path.exists(\"./2_Vit_LSTM/model_trained_ViT_LSTM_att/my_model.data-00000-of-00001\"):\n","        print(\"deleting ./2_Vit_LSTM/model_trained_ViT_LSTM_att/my_model.data-00000-of-00001\")\n","        os.unlink(\"./2_Vit_LSTM/model_trained_ViT_LSTM_att/my_model.data-00000-of-00001\")\n","    if os.path.exists(\"./2_Vit_LSTM/model_trained_ViT_LSTM_att/learning_curve.png\"):\n","        print(\"deleting ./2_Vit_LSTM/model_trained_ViT_LSTM_att/learning_curve.png\")\n","        os.unlink(\"./2_Vit_LSTM/model_trained_ViT_LSTM_att/learning_curve.png\")\n","\n","ckpt = tf.train.Checkpoint(encoder=encoder,\n","                           decoder=decoder,\n","                           optimizer=optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","start_epoch = 0\n","if ckpt_manager.latest_checkpoint:\n","    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","    print('start_epoch', start_epoch)\n","    # restoring the latest checkpoint in checkpoint_path\n","    ckpt.restore(ckpt_manager.latest_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8013235,"status":"ok","timestamp":1701801826476,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"MMZnaCxvR4ah","outputId":"ba46e681-784e-4c49-c969-b31fed20d447"},"outputs":[{"output_type":"stream","name":"stdout","text":["training..\n","Epoch 1 Batch 0 Loss 2.0664\n","Epoch 1 Batch 100 Loss 1.2629\n","Epoch 1 Batch 200 Loss 1.1095\n","Epoch 1 Batch 300 Loss 0.9351\n","Epoch 1 Batch 400 Loss 0.9025\n","Epoch 1 Batch 500 Loss 0.8529\n","Epoch 1 Batch 600 Loss 0.8724\n","Epoch 1 Batch 700 Loss 0.8227\n","Epoch 1 Batch 800 Loss 0.7473\n","Epoch 1 Batch 900 Loss 0.8550\n","Epoch 1 Batch 1000 Loss 0.6806\n","Epoch 1 Batch 1100 Loss 0.7795\n","Epoch 1 Batch 1200 Loss 0.8872\n","Epoch 1 Batch 1300 Loss 0.7872\n","Epoch 1 Batch 1400 Loss 0.7702\n","Epoch 1 Batch 1500 Loss 0.7758\n","Epoch 1 Batch 1600 Loss 0.6875\n","Epoch 1 Batch 1700 Loss 0.7372\n","Epoch 1 Batch 1800 Loss 0.7443\n","Epoch 1 Batch 1900 Loss 0.7056\n","Epoch 1 Batch 2000 Loss 0.6797\n","Epoch 1 Batch 2100 Loss 0.7640\n","Epoch 1 Batch 2200 Loss 0.7098\n","Epoch 1 Loss 0.816579\n","Time taken for 1 epoch 2560.551245212555 sec\n","\n","Epoch 2 Batch 0 Loss 0.6358\n","Epoch 2 Batch 100 Loss 0.6553\n","Epoch 2 Batch 200 Loss 0.6021\n","Epoch 2 Batch 300 Loss 0.6947\n","Epoch 2 Batch 400 Loss 0.7516\n","Epoch 2 Batch 500 Loss 0.6775\n","Epoch 2 Batch 600 Loss 0.6336\n","Epoch 2 Batch 700 Loss 0.6791\n","Epoch 2 Batch 800 Loss 0.6484\n","Epoch 2 Batch 900 Loss 0.6670\n","Epoch 2 Batch 1000 Loss 0.5816\n","Epoch 2 Batch 1100 Loss 0.6363\n","Epoch 2 Batch 1200 Loss 0.5483\n","Epoch 2 Batch 1300 Loss 0.6985\n","Epoch 2 Batch 1400 Loss 0.7443\n","Epoch 2 Batch 1500 Loss 0.6254\n","Epoch 2 Batch 1600 Loss 0.6795\n","Epoch 2 Batch 1700 Loss 0.7097\n","Epoch 2 Batch 1800 Loss 0.5694\n","Epoch 2 Batch 1900 Loss 0.5798\n","Epoch 2 Batch 2000 Loss 0.6026\n","Epoch 2 Batch 2100 Loss 0.6257\n","Epoch 2 Batch 2200 Loss 0.5598\n","Epoch 2 Loss 0.650494\n","Time taken for 1 epoch 606.6900918483734 sec\n","\n","Epoch 3 Batch 0 Loss 0.6243\n","Epoch 3 Batch 100 Loss 0.6049\n","Epoch 3 Batch 200 Loss 0.6423\n","Epoch 3 Batch 300 Loss 0.5949\n","Epoch 3 Batch 400 Loss 0.5896\n","Epoch 3 Batch 500 Loss 0.5551\n","Epoch 3 Batch 600 Loss 0.5863\n","Epoch 3 Batch 700 Loss 0.6082\n","Epoch 3 Batch 800 Loss 0.5505\n","Epoch 3 Batch 900 Loss 0.6010\n","Epoch 3 Batch 1000 Loss 0.5687\n","Epoch 3 Batch 1100 Loss 0.5603\n","Epoch 3 Batch 1200 Loss 0.4903\n","Epoch 3 Batch 1300 Loss 0.5821\n","Epoch 3 Batch 1400 Loss 0.5761\n","Epoch 3 Batch 1500 Loss 0.5773\n","Epoch 3 Batch 1600 Loss 0.6365\n","Epoch 3 Batch 1700 Loss 0.5233\n","Epoch 3 Batch 1800 Loss 0.5380\n","Epoch 3 Batch 1900 Loss 0.5180\n","Epoch 3 Batch 2000 Loss 0.5929\n","Epoch 3 Batch 2100 Loss 0.6252\n","Epoch 3 Batch 2200 Loss 0.5546\n","Epoch 3 Loss 0.597248\n","Time taken for 1 epoch 606.5695700645447 sec\n","\n","Epoch 4 Batch 0 Loss 0.6281\n","Epoch 4 Batch 100 Loss 0.5610\n","Epoch 4 Batch 200 Loss 0.5356\n","Epoch 4 Batch 300 Loss 0.5540\n","Epoch 4 Batch 400 Loss 0.5959\n","Epoch 4 Batch 500 Loss 0.6437\n","Epoch 4 Batch 600 Loss 0.5852\n","Epoch 4 Batch 700 Loss 0.5824\n","Epoch 4 Batch 800 Loss 0.4981\n","Epoch 4 Batch 900 Loss 0.5259\n","Epoch 4 Batch 1000 Loss 0.6480\n","Epoch 4 Batch 1100 Loss 0.5930\n","Epoch 4 Batch 1200 Loss 0.6098\n","Epoch 4 Batch 1300 Loss 0.6167\n","Epoch 4 Batch 1400 Loss 0.5353\n","Epoch 4 Batch 1500 Loss 0.5311\n","Epoch 4 Batch 1600 Loss 0.5403\n","Epoch 4 Batch 1700 Loss 0.5216\n","Epoch 4 Batch 1800 Loss 0.4658\n","Epoch 4 Batch 1900 Loss 0.4594\n","Epoch 4 Batch 2000 Loss 0.5175\n","Epoch 4 Batch 2100 Loss 0.5715\n","Epoch 4 Batch 2200 Loss 0.4867\n","Epoch 4 Loss 0.558841\n","Time taken for 1 epoch 604.8443267345428 sec\n","\n","Epoch 5 Batch 0 Loss 0.5245\n","Epoch 5 Batch 100 Loss 0.5356\n","Epoch 5 Batch 200 Loss 0.5931\n","Epoch 5 Batch 300 Loss 0.5443\n","Epoch 5 Batch 400 Loss 0.5510\n","Epoch 5 Batch 500 Loss 0.6107\n","Epoch 5 Batch 600 Loss 0.5166\n","Epoch 5 Batch 700 Loss 0.5215\n","Epoch 5 Batch 800 Loss 0.5031\n","Epoch 5 Batch 900 Loss 0.5868\n","Epoch 5 Batch 1000 Loss 0.5223\n","Epoch 5 Batch 1100 Loss 0.5332\n","Epoch 5 Batch 1200 Loss 0.5714\n","Epoch 5 Batch 1300 Loss 0.5177\n","Epoch 5 Batch 1400 Loss 0.5412\n","Epoch 5 Batch 1500 Loss 0.4980\n","Epoch 5 Batch 1600 Loss 0.4368\n","Epoch 5 Batch 1700 Loss 0.5011\n","Epoch 5 Batch 1800 Loss 0.5309\n","Epoch 5 Batch 1900 Loss 0.5820\n","Epoch 5 Batch 2000 Loss 0.5972\n","Epoch 5 Batch 2100 Loss 0.5028\n","Epoch 5 Batch 2200 Loss 0.5249\n","Epoch 5 Loss 0.526572\n","Time taken for 1 epoch 605.621741771698 sec\n","\n","Epoch 6 Batch 0 Loss 0.5065\n","Epoch 6 Batch 100 Loss 0.5422\n","Epoch 6 Batch 200 Loss 0.5890\n","Epoch 6 Batch 300 Loss 0.4922\n","Epoch 6 Batch 400 Loss 0.4822\n","Epoch 6 Batch 500 Loss 0.5575\n","Epoch 6 Batch 600 Loss 0.4970\n","Epoch 6 Batch 700 Loss 0.4784\n","Epoch 6 Batch 800 Loss 0.5101\n","Epoch 6 Batch 900 Loss 0.5838\n","Epoch 6 Batch 1000 Loss 0.4996\n","Epoch 6 Batch 1100 Loss 0.4773\n","Epoch 6 Batch 1200 Loss 0.5163\n","Epoch 6 Batch 1300 Loss 0.5030\n","Epoch 6 Batch 1400 Loss 0.5579\n","Epoch 6 Batch 1500 Loss 0.4461\n","Epoch 6 Batch 1600 Loss 0.4325\n","Epoch 6 Batch 1700 Loss 0.4834\n","Epoch 6 Batch 1800 Loss 0.4604\n","Epoch 6 Batch 1900 Loss 0.4609\n","Epoch 6 Batch 2000 Loss 0.5825\n","Epoch 6 Batch 2100 Loss 0.5456\n","Epoch 6 Batch 2200 Loss 0.4840\n","Epoch 6 Loss 0.498433\n","Time taken for 1 epoch 606.0320727825165 sec\n","\n","Epoch 7 Batch 0 Loss 0.5469\n","Epoch 7 Batch 100 Loss 0.4522\n","Epoch 7 Batch 200 Loss 0.5473\n","Epoch 7 Batch 300 Loss 0.5082\n","Epoch 7 Batch 400 Loss 0.5789\n","Epoch 7 Batch 500 Loss 0.5120\n","Epoch 7 Batch 600 Loss 0.4001\n","Epoch 7 Batch 700 Loss 0.4230\n","Epoch 7 Batch 800 Loss 0.4251\n","Epoch 7 Batch 900 Loss 0.5127\n","Epoch 7 Batch 1000 Loss 0.4175\n","Epoch 7 Batch 1100 Loss 0.4653\n","Epoch 7 Batch 1200 Loss 0.4752\n","Epoch 7 Batch 1300 Loss 0.4770\n","Epoch 7 Batch 1400 Loss 0.4290\n","Epoch 7 Batch 1500 Loss 0.4995\n","Epoch 7 Batch 1600 Loss 0.4619\n","Epoch 7 Batch 1700 Loss 0.4430\n","Epoch 7 Batch 1800 Loss 0.4419\n","Epoch 7 Batch 1900 Loss 0.4817\n","Epoch 7 Batch 2000 Loss 0.5430\n","Epoch 7 Batch 2100 Loss 0.3873\n","Epoch 7 Batch 2200 Loss 0.4566\n","Epoch 7 Loss 0.473426\n","Time taken for 1 epoch 605.4669787883759 sec\n","\n","Epoch 8 Batch 0 Loss 0.4763\n","Epoch 8 Batch 100 Loss 0.5129\n","Epoch 8 Batch 200 Loss 0.4774\n","Epoch 8 Batch 300 Loss 0.4530\n","Epoch 8 Batch 400 Loss 0.5212\n","Epoch 8 Batch 500 Loss 0.4634\n","Epoch 8 Batch 600 Loss 0.4783\n","Epoch 8 Batch 700 Loss 0.4535\n","Epoch 8 Batch 800 Loss 0.4302\n","Epoch 8 Batch 900 Loss 0.4126\n","Epoch 8 Batch 1000 Loss 0.4449\n","Epoch 8 Batch 1100 Loss 0.5049\n","Epoch 8 Batch 1200 Loss 0.5251\n","Epoch 8 Batch 1300 Loss 0.5047\n","Epoch 8 Batch 1400 Loss 0.4758\n","Epoch 8 Batch 1500 Loss 0.4175\n","Epoch 8 Batch 1600 Loss 0.4683\n","Epoch 8 Batch 1700 Loss 0.5203\n","Epoch 8 Batch 1800 Loss 0.5308\n","Epoch 8 Batch 1900 Loss 0.4671\n","Epoch 8 Batch 2000 Loss 0.4006\n","Epoch 8 Batch 2100 Loss 0.4074\n","Epoch 8 Batch 2200 Loss 0.4788\n","Epoch 8 Loss 0.450781\n","Time taken for 1 epoch 605.6150343418121 sec\n","\n","Epoch 9 Batch 0 Loss 0.4211\n","Epoch 9 Batch 100 Loss 0.4768\n","Epoch 9 Batch 200 Loss 0.4082\n","Epoch 9 Batch 300 Loss 0.4214\n","Epoch 9 Batch 400 Loss 0.4348\n","Epoch 9 Batch 500 Loss 0.4511\n","Epoch 9 Batch 600 Loss 0.3872\n","Epoch 9 Batch 700 Loss 0.4665\n","Epoch 9 Batch 800 Loss 0.4079\n","Epoch 9 Batch 900 Loss 0.3856\n","Epoch 9 Batch 1000 Loss 0.3925\n","Epoch 9 Batch 1100 Loss 0.3795\n","Epoch 9 Batch 1200 Loss 0.3956\n","Epoch 9 Batch 1300 Loss 0.3968\n","Epoch 9 Batch 1400 Loss 0.4617\n","Epoch 9 Batch 1500 Loss 0.3792\n","Epoch 9 Batch 1600 Loss 0.3899\n","Epoch 9 Batch 1700 Loss 0.4289\n","Epoch 9 Batch 1800 Loss 0.4407\n","Epoch 9 Batch 1900 Loss 0.3783\n","Epoch 9 Batch 2000 Loss 0.4402\n","Epoch 9 Batch 2100 Loss 0.4106\n","Epoch 9 Batch 2200 Loss 0.3572\n","Epoch 9 Loss 0.430457\n","Time taken for 1 epoch 605.3484544754028 sec\n","\n","Epoch 10 Batch 0 Loss 0.3950\n","Epoch 10 Batch 100 Loss 0.4163\n","Epoch 10 Batch 200 Loss 0.4194\n","Epoch 10 Batch 300 Loss 0.4003\n","Epoch 10 Batch 400 Loss 0.4652\n","Epoch 10 Batch 500 Loss 0.4230\n","Epoch 10 Batch 600 Loss 0.4470\n","Epoch 10 Batch 700 Loss 0.4064\n","Epoch 10 Batch 800 Loss 0.4313\n","Epoch 10 Batch 900 Loss 0.4292\n","Epoch 10 Batch 1000 Loss 0.4532\n","Epoch 10 Batch 1100 Loss 0.4590\n","Epoch 10 Batch 1200 Loss 0.3927\n","Epoch 10 Batch 1300 Loss 0.4523\n","Epoch 10 Batch 1400 Loss 0.4349\n","Epoch 10 Batch 1500 Loss 0.3697\n","Epoch 10 Batch 1600 Loss 0.3977\n","Epoch 10 Batch 1700 Loss 0.3807\n","Epoch 10 Batch 1800 Loss 0.4075\n","Epoch 10 Batch 1900 Loss 0.3911\n","Epoch 10 Batch 2000 Loss 0.4283\n","Epoch 10 Batch 2100 Loss 0.3932\n","Epoch 10 Batch 2200 Loss 0.3515\n","Epoch 10 Loss 0.412471\n","Time taken for 1 epoch 605.4908714294434 sec\n","\n","time of training:  8012.231321811676\n"]}],"source":["\"\"\"## Training\n","\n","\"\"\"\n","\n","# adding this in a separate cell because if you run the training cell\n","# many times, the loss_plot array will be reset\n","loss_plot = []\n","\n","\n","@tf.function\n","def train_step(img_tensor, target):\n","    loss = 0\n","\n","    # initializing the hidden state for each batch\n","    # because the captions are not related from image to image\n","    hidden = decoder.reset_state(batch_size=target.shape[0])\n","\n","    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n","\n","    with tf.GradientTape() as tape:\n","        features = encoder(img_tensor)\n","        # print('encoder', encoder.count_params())\n","\n","        for i in range(1, target.shape[1]):\n","            # passing the features through the decoder\n","            predictions, hidden, _ = decoder(dec_input, features, hidden)\n","            # print('decoder', decoder.count_params())\n","\n","            loss += loss_function(target[:, i], predictions)\n","\n","            # using teacher forcing\n","            dec_input = tf.expand_dims(target[:, i], 1)\n","\n","    total_loss = (loss / int(target.shape[1]))\n","\n","    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","    gradients = tape.gradient(loss, trainable_variables)\n","\n","    optimizer.apply_gradients(zip(gradients, trainable_variables))\n","\n","    del hidden\n","    del dec_input\n","    del features\n","    del predictions\n","    del trainable_variables\n","    gc.collect()\n","\n","    return loss, total_loss\n","\n","\n","# EPOCHS = EPOCH_COUNT  # hadie\n","EPOCHS =10\n","\n","if not os.path.exists(\"./2_Vit_LSTM/model_trained_ViT_LSTM_att/my_model.index\"):\n","    print(\"training..\")  # hadie\n","\n","    t1 = time.time()\n","    for epoch in range(start_epoch, EPOCHS):\n","        start = time.time()\n","        total_loss = 0\n","\n","        for (batch, (img_tensor, target)) in enumerate(dataset):\n","            # print('img_tensor.shape', img_tensor.shape)\n","            # print('target', target.shape)\n","            batch_loss, t_loss = train_step(img_tensor, target)\n","            total_loss += t_loss\n","\n","            if batch % 100 == 0:\n","                print ('Epoch {} Batch {} Loss {:.4f}'.format(\n","                  epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n","        # storing the epoch end loss value to plot later\n","        loss_plot.append(total_loss / num_steps)\n","\n","        # if epoch % 5 == 0:\n","        ckpt_manager.save()\n","\n","        print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n","                                             total_loss / num_steps))\n","        print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","    t2 = time.time()\n","\n","    # save model to disk # hadie\n","    decoder.save_weights(\"./2_Vit_LSTM/model_trained_ViT_LSTM_att/my_model\" , save_format=\"tf\")  # hadie\n","    print('time of training: ', t2-t1)\n","else:  # hadie\n","    print(\"A trained model has been found. Loading it from disk..\")  # hadie\n","    # Load the previously saved weights # hadie\n","    decoder.load_weights(\"./2_Vit_LSTM/model_trained_ViT_LSTM_att/my_model\")  # hadie"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1znEkYuB2If1q0NgULfz_XeUlHImY3DDs"},"executionInfo":{"elapsed":33550,"status":"ok","timestamp":1701801860013,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"r_8xSHL6z1wE","outputId":"808eb706-afff-4035-e942-0ca54ad0b66e"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["\"\"\"## Caption!\n","\n","\"\"\"\n","attention_features_shape = 197\n","\n","def evaluate(image):\n","    attention_plot = np.zeros((max_length, attention_features_shape))\n","    # print(image)\n","    combined_features = extract_vit_feature(image)\n","    # print('combined_features.shape', combined_features.shape)\n","    features = encoder(combined_features)  # hadie\n","\n","    hidden = decoder.reset_state(batch_size=1)\n","    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n","\n","    result = []\n","\n","    for i in range(max_length):\n","        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n","        attention_plot[i] = tf.reshape(attention_weights, (-1,)).numpy()\n","\n","        # predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","        predicted_id = np.argmax(predictions.numpy())\n","\n","        result.append(tokenizer.index_word[predicted_id])\n","\n","        if tokenizer.index_word[predicted_id] == '<end>':\n","            return result, attention_plot\n","\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    attention_plot = attention_plot[:len(result),:]\n","    return result, attention_plot\n","\n","\n","def plot_attention(image, result, attention_plot):\n","    temp_image = np.array(Image.open(image))\n","\n","    fig = plt.figure(figsize=(10, 10))\n","\n","    len_result = len(result)\n","    for l in range(len_result):\n","        temp_att = np.resize(attention_plot[l], (8, 8))\n","        ax = fig.add_subplot(len_result // 2, len_result // 2, l + 1)\n","        ax.set_title(result[l])\n","        img = ax.imshow(temp_image)\n","        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def list_to_dict(name_list, caption_list):\n","    name_dict = {}\n","    for i in range(len(name_list)):\n","        if name_list[i] not in name_dict:\n","            name_dict[name_list[i]] = [caption_list[i]]\n","        else:\n","            name_dict.get(name_list[i]).append(caption_list[i])\n","            # print( name_dict.get(name_list[i]))\n","    return name_dict\n","\n","\n","# captions on the validation set\n","val_dict = list_to_dict(img_name_val, cap_val)\n","\n","from google.colab.patches import cv2_imshow\n","\n","for i in range(10):\n","    # rid = np.random.randint(0, len(img_name_val))\n","    image = img_name_val[i*5]\n","\n","    # real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n","    captions = val_dict[image]\n","\n","    result, attention_plot = evaluate(image)\n","\n","    # print ('Real Caption:', real_caption)\n","    print('Real captions:')\n","    for j in range(len(captions)):\n","        real_caption = ' '.join([tokenizer.index_word[i] for i in captions[j] if i not in [0]])\n","        print(real_caption)\n","\n","    print ('Prediction Caption:', ' '.join(result))\n","    # plot_attention(image, result, attention_plot)\n","\n","    print('img path', image)\n","\n","    img = cv2.imread(image)\n","    cv2_imshow(img)\n","\n","# list_img = [\n","#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000117179.jpg',\n","#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000335744.jpg',\n","#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000033435.jpg',\n","#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000555896.jpg',\n","#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000448069.jpg',\n","#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000367171.jpg',\n","#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000039758.jpg',\n","#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000192145.jpg',\n","#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000493483.jpg',\n","#     '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000519404.jpg'\n","\n","# ]\n","\n","# for image in list_img:\n","#     # real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n","#     captions = val_dict[image]\n","\n","#     result, attention_plot = evaluate(image)\n","\n","#     # print ('Real Caption:', real_caption)\n","#     print('Real captions:')\n","#     for j in range(len(captions)):\n","#         real_caption = ' '.join([tokenizer.index_word[i] for i in captions[j] if i not in [0]])\n","#         print(real_caption)\n","\n","#     print ('Prediction Caption:', ' '.join(result))\n","#     # plot_attention(image, result, attention_plot)\n","\n","#     print('img path', image)\n","\n","#     img = cv2.imread(image)\n","#     cv2_imshow(img)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4393820,"status":"ok","timestamp":1701806253824,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"FRVTZv2IkjHV","outputId":"c9941b56-1a6e-4946-9194-a49e4f8424c6"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","100%|██████████| 2000/2000 [1:13:18<00:00,  2.20s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","bleu-1 score: 0.5759271554585981\n","bleu-2 score: 0.41538268949698104\n","bleu-3 score: 0.32449185708360434\n","bleu-4 score: 0.1929726389342297\n","meteor score: 0.4029793499999997\n"]}],"source":["from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n","from nltk.translate.meteor_score import meteor_score\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","\n","def img_to_caption(img_name):\n","    result, attention_plot = evaluate(img_name)\n","    result = ' '.join(result).replace(\"<end>\", \"\").strip()\n","    return result\n","\n","actual, predicted = [], []\n","\n","for img_name in tqdm(val_dict):\n","\n","    yhat = img_to_caption(img_name)\n","    pred = yhat.split()\n","\n","    actu = []\n","    captions = val_dict[img_name]\n","    for i in range(len(captions)):\n","        real_caption = ' '.join([tokenizer.index_word[i] for i in captions[i] if i not in [0,3,4]])\n","        references = real_caption.split(' ')\n","        actu.append(references)\n","\n","    actual.append(actu)\n","    predicted.append(pred)\n","\n","bl1 = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n","bl2 = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n","bl3 = corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))\n","bl4 = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n","\n","mt = 0\n","for i in range(len(predicted)):\n","    mt += round(meteor_score(actual[i], predicted[i]),4)\n","mt = mt/len(predicted)\n","\n","print(\"\\nbleu-1 score:\", bl1)\n","print(\"bleu-2 score:\", bl2)\n","print(\"bleu-3 score:\", bl3)\n","print(\"bleu-4 score:\", bl4)\n","print(\"meteor score:\", mt)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":143497,"status":"ok","timestamp":1701806397314,"user":{"displayName":"Hong Dao","userId":"07852252750226172275"},"user_tz":-420},"id":"TeYcsfVNoT0P","outputId":"f3ac614e-d446-4440-e23c-2ddec46983a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["time of predicting captions for 100 images: 143.15592646598816\n"]}],"source":["# speed test\n","\n","# for random samples\n","img_list = []\n","for i in range(100):\n","    rid = np.random.randint(0, len(img_name_val))\n","    img_list.append(img_name_val[rid])\n","\n","t1 = time.time()\n","for image in img_list:\n","    result, attention_plot = evaluate(image)\n","    predict = ' '.join(result)\n","\n","t2 = time.time()\n","print('time of predicting captions for 100 images:', t2-t1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxtTsgyO70G9"},"outputs":[],"source":["# # speed test => moved to local test\n","\n","# file = open('./save/img_name_val_bk.txt', 'r')\n","# txt = file.read()\n","# file.close()\n","\n","# test_imgs1 = txt[1:-1].split(\", \")\n","# test_imgs = []\n","# num_test_file = 5\n","# for i in range(num_test_file):\n","#     test_imgs.append(test_imgs1[i][1:-1])\n","\n","# # ['/content/drive/MyDrive/coco/train2014/COCO_train2014_000000151458.jpg',\n","# # '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000419159.jpg',\n","# # '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000239584.jpg',\n","# # '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000521266.jpg',\n","# # '/content/drive/MyDrive/coco/train2014/COCO_train2014_000000337415.jpg']\n","\n","# time_extract = 0\n","# time_predict = 0\n","\n","# def predict_caption(image):\n","\n","#     time_start_extract = time.time()\n","#     combined_features = extract_combined_feature(image)\n","#     t = time.time() - time_start_extract\n","\n","#     features = encoder(combined_features)\n","\n","#     hidden = decoder.reset_state(batch_size=1)\n","#     dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n","\n","#     result = []\n","#     for i in range(max_length):\n","#         predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n","\n","#         predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","\n","#         # hong\n","#         while predicted_id > len(tokenizer.index_word)-1:\n","#             predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","#         # hong end\n","\n","#         result.append(tokenizer.index_word[predicted_id])\n","\n","#         if tokenizer.index_word[predicted_id] == '<end>':\n","#             # print('early')\n","#             return result, t\n","\n","#         dec_input = tf.expand_dims([predicted_id], 0)\n","\n","#     return result, t\n","\n","\n","# # captions on the validation set\n","# n = len(test_imgs)\n","# for i in range(n):\n","#     image = test_imgs[i]\n","\n","#     time_start_predict = time.time()\n","#     result, t = predict_caption(image)\n","#     time_predict += time.time() - time_start_predict\n","#     time_extract += t\n","\n","#     print ('Prediction Caption:', ' '.join(result))\n","\n","#     # print('img path', image)\n","#     # import cv2\n","#     # from google.colab.patches import cv2_imshow\n","#     # img = cv2.imread(image)\n","#     # cv2_imshow(img)\n","\n","# time_extract /= n\n","# time_predict /= n\n","# print('average time_extract by Yolo_RNN', time_extract)\n","# print('average time_predict by Yolo_RNN', time_predict)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNJJwfnO4CZsoJ6naa++5wL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"620cfda95720483fb9a6083328226a22":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7560884585fb4233b17c8aacc902c2b3","IPY_MODEL_0c1352f6614941a39a1cc3231968b0f3","IPY_MODEL_742ac464ead34febac1332e6ad7f19cc"],"layout":"IPY_MODEL_a5a8d04c15f44cb9a06fa324e620dc7c"}},"7560884585fb4233b17c8aacc902c2b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_950971fee9b94963a0c5a59ec13322a6","placeholder":"​","style":"IPY_MODEL_febec9b7d2624c2ca813053376710e88","value":"preprocessor_config.json: 100%"}},"0c1352f6614941a39a1cc3231968b0f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b76370f6b7145a8b445f67d76bc9551","max":160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6000fef7c85b40c4a5bdc2c3671e1d77","value":160}},"742ac464ead34febac1332e6ad7f19cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fe2fc8692df4fbea60e7f8f5bc1f272","placeholder":"​","style":"IPY_MODEL_afebf6384f0b47d8b590832c9c40bb90","value":" 160/160 [00:00&lt;00:00, 11.1kB/s]"}},"a5a8d04c15f44cb9a06fa324e620dc7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"950971fee9b94963a0c5a59ec13322a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"febec9b7d2624c2ca813053376710e88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b76370f6b7145a8b445f67d76bc9551":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6000fef7c85b40c4a5bdc2c3671e1d77":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fe2fc8692df4fbea60e7f8f5bc1f272":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afebf6384f0b47d8b590832c9c40bb90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7beff1706b2d400ab6a33ae3588ec536":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_805526378c4f4c8b92d9d6fc51a78e9c","IPY_MODEL_c68c58b489b6434e9363827abf9b0819","IPY_MODEL_ef04257b625c455cbb6f022109ac0b7a"],"layout":"IPY_MODEL_3acbe04345ca4f839bbe50d1f11d57b6"}},"805526378c4f4c8b92d9d6fc51a78e9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b6fcaa0d7af4a0999ca9c42d5efd98b","placeholder":"​","style":"IPY_MODEL_c3e734ae148c410388fa40bb4c12d84e","value":"config.json: 100%"}},"c68c58b489b6434e9363827abf9b0819":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cd24d363817444dba956e8060687015","max":502,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65c4fc7971f04b07b6ae62018facac78","value":502}},"ef04257b625c455cbb6f022109ac0b7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36d99f10912d4a6987c9fd99bc4fc06c","placeholder":"​","style":"IPY_MODEL_510ef5aee90449ac9d65875536f73bd8","value":" 502/502 [00:00&lt;00:00, 39.2kB/s]"}},"3acbe04345ca4f839bbe50d1f11d57b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b6fcaa0d7af4a0999ca9c42d5efd98b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3e734ae148c410388fa40bb4c12d84e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5cd24d363817444dba956e8060687015":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65c4fc7971f04b07b6ae62018facac78":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"36d99f10912d4a6987c9fd99bc4fc06c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"510ef5aee90449ac9d65875536f73bd8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c125eda82e944a79d4c62338ea76ea9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_59d322b641a9422b88e53dbe85081877","IPY_MODEL_9e99f84bbe7f409eb610af7fed2b4f7b","IPY_MODEL_45541e26638b4f0a8f368704ff8c4c32"],"layout":"IPY_MODEL_a318b2d7fff1464db6772e3246c21e3f"}},"59d322b641a9422b88e53dbe85081877":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66dd3853320049f7a76d020e434dd305","placeholder":"​","style":"IPY_MODEL_7a38378a2678497d92f6ca0ceda24203","value":"pytorch_model.bin: 100%"}},"9e99f84bbe7f409eb610af7fed2b4f7b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce7eecd7972c41a58cc039d7a2b5312f","max":345636463,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9732c270980f41ce952fe38907f60791","value":345636463}},"45541e26638b4f0a8f368704ff8c4c32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4be5d015c59743dab87b55847b3c59c9","placeholder":"​","style":"IPY_MODEL_57dca04963004400bee0003c75c4e701","value":" 346M/346M [00:00&lt;00:00, 418MB/s]"}},"a318b2d7fff1464db6772e3246c21e3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66dd3853320049f7a76d020e434dd305":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a38378a2678497d92f6ca0ceda24203":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce7eecd7972c41a58cc039d7a2b5312f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9732c270980f41ce952fe38907f60791":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4be5d015c59743dab87b55847b3c59c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57dca04963004400bee0003c75c4e701":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}